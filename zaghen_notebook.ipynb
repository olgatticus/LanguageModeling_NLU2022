{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "zaghen_224436.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ">     Instructions for the execution of the code:\n",
        ">     \n",
        ">     - We suppose to work with an existing directory with path \"/content/gdrive/MyDrive/NLU_project/\", \n",
        ">       containing a folder \"dataset\" with the zipped dataset ptbdataset.zip (at \"dataset/ptbdataset.zip\") and an empty folder \"models\". \n",
        ">     - The code cells should be executed one after the other, in the same order in which they appear. \n",
        ">       Otherwise some errors may arise in the execution."
      ],
      "metadata": {
        "id": "HYS2nwr35KNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<center> Language Modeling </center>\n",
        "###### <center> with </center>\n",
        "#<center> LSTM and Attention </center>\n",
        "---"
      ],
      "metadata": {
        "id": "VHGZkAGW6Bqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-       Imports and global variables"
      ],
      "metadata": {
        "id": "L-z8zreZ_v0F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CeXfbUjf9EzC"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "import nltk\n",
        "\n",
        "\n",
        "# Seeds\n",
        "torch.manual_seed(5)\n",
        "random.seed(5)\n",
        "np.random.seed(5)\n",
        "\n",
        "# Global variables\n",
        "device = 'cuda:0' \n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "PAD_TOKEN = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-       Mounting of the Google Drive folder"
      ],
      "metadata": {
        "id": "A33zp1RmBVxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "FEynazgR-Wuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94694856-27d2-4f52-d9f2-f83f809ef941"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "## <center>Analysis of the Penn Tree bank dataset:</center>\n",
        "This section contains all the operations performed on the dataset with the aims of:\n",
        "-     extracting all the training, test and validation sentences\n",
        "-     constructing a vocabulary and a map from words to indexes (and viceversa)\n",
        "-     analyzing the properties of the dataset. "
      ],
      "metadata": {
        "id": "Gl_pU2Js-XTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/MyDrive/NLU_project/dataset/ptbdataset.zip -d gdrive/MyDrive/NLU_project/dataset"
      ],
      "metadata": {
        "id": "o4cE6X97-iZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Extraction of the sentences and the words from the dataset\n",
        "\n",
        "The dataset consists in three .txt files, corresponding to training, validation and test sets, each of them containing sentences, distinguished from being in different line of the file."
      ],
      "metadata": {
        "id": "doytZhzhv02L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function that returns all the sentences in the txt from the input path.\n",
        "\n",
        ":param path: path to the txt dataset\n",
        ":return sents: list of all sentences\n",
        "\"\"\"\n",
        "\n",
        "def list_sents(path):\n",
        "    sents = []\n",
        "    with open(path) as file:\n",
        "        for line in file:\n",
        "          sent = line.split()\n",
        "          sents.append(sent)\n",
        "    file.close()\n",
        "    return sents\n",
        "\n",
        "\"\"\"\n",
        "Function that returns all the words in the txt of the input path.\n",
        "\n",
        ":param path: path to the txt dataset\n",
        ":return sents: set of all words\n",
        "\"\"\"\n",
        "def list_words(path):\n",
        "    words = []\n",
        "    with open(path) as file:\n",
        "        for line in file:\n",
        "          for word in line.split():\n",
        "            words.append(word)\n",
        "    file.close()\n",
        "    return words"
      ],
      "metadata": {
        "id": "RGKZecmfv6cV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Extraction of sentences from the dataset, seen as lists of words."
      ],
      "metadata": {
        "id": "MX_1WtejQmEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentences in the training set \n",
        "train_sents = list_sents(\"gdrive/MyDrive/NLU_project/dataset/ptb.train.txt\")\n",
        "\n",
        "# Sentences in the test set \n",
        "test_sents = list_sents(\"gdrive/MyDrive/NLU_project/dataset/ptb.test.txt\")\n",
        "\n",
        "# Sentences in the validation set \n",
        "valid_sents = list_sents(\"gdrive/MyDrive/NLU_project/dataset/ptb.valid.txt\")"
      ],
      "metadata": {
        "id": "jr7OLwp3xKfg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*    Extraction of the list (and set) of words from the dataset."
      ],
      "metadata": {
        "id": "52tgZNFUQvrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Words in the training set \n",
        "train_words_list = list_words(\"gdrive/MyDrive/NLU_project/dataset/ptb.train.txt\")\n",
        "train_words = set(train_words_list)\n",
        "\n",
        "# Words in the test set \n",
        "test_words_list = list_words(\"gdrive/MyDrive/NLU_project/dataset/ptb.test.txt\")\n",
        "test_words = set(test_words_list)\n",
        "\n",
        "# Words in the validation set \n",
        "valid_words_list = list_words(\"gdrive/MyDrive/NLU_project/dataset/ptb.valid.txt\")\n",
        "valid_words = set(valid_words_list)"
      ],
      "metadata": {
        "id": "oIXI5jw00Cnv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "At this point some considerations can be made about the dataset, that are useful for the preprocessing as well as for the setting of the training procedure:\n",
        "\n",
        "-      all the words it contains are already **lowercased**\n",
        "-      it does not contain puctuation\n",
        "-      all the numbers have been replaced with the symbol \"**N**\"\n",
        "-      the training set, as well as the test and validation set, contain the special token **\\<unk\\>**. This means that this dataset has already been preprocessed and that the \\<unk\\> token is already applied in the corpus, because the maximum vocabulary size was previously fixed to **10k words**.\n"
      ],
      "metadata": {
        "id": "jXgIt8XqHAg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The <unk> token is already contained\n",
        "print('<unk>' in train_words)\n",
        "\n",
        "# The numbers are replaced with N\n",
        "print('N' in train_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bxd_gnMJNbS",
        "outputId": "20fe6af2-5e09-4600-9bb3-0be9b9fcc4c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "Another property that must be checked is the maximum length of the sentences in the training set. Indeed, since the Language Model is implemented through an RNN architecture, it's important to make sure the training sequences are not excessively long, otherwise the simple Backpropagation Through Time may suffer of vanishing or exploding gradients and a more sophisticated approach (Truncated BBTT) should be adopted to reduce the problem.\n",
        "\n",
        "It's easily verified that the maximum length of the sentences in the training set is 82. This guarantees that Backpropagation Through Time can be adopted without too serious consequences in the following steps.\n",
        "\n",
        "Also, the length of each sequence in **train_sents** is at least 1. Indeed, it is possibile to verify that none of the sentences has length 0 (which means that there are no empty lines in the .txt file), while many of them have length 1, and they correspond to sentences composed by one single word."
      ],
      "metadata": {
        "id": "XV96qY5OlsOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The sequences with length values over 80 are few\n",
        "print(\"Maximum length reached in training set: \")\n",
        "max_len = -1\n",
        "for sent in train_sents:\n",
        "  if len(sent) > max_len:\n",
        "    max_len = len(sent)\n",
        "print(max_len)\n",
        "\n",
        "# The sequences with length values equal to 1 are a lot\n",
        "print(\"\\nNumber of sentences with length equal to 1 in training set: \")\n",
        "sent_equal_1 = 0\n",
        "for sent in train_sents:\n",
        "  if len(sent) == 1:\n",
        "    sent_equal_1 += 1\n",
        "print(sent_equal_1)\n",
        "\n",
        "# There are no sequences of length 0\n",
        "print(\"\\nNumber of sentences with length equal to 0 in training set: \")\n",
        "sent_equal_0 = 0\n",
        "for sent in train_sents:\n",
        "  if len(sent) == 0:\n",
        "    sent_equal_0 += 1\n",
        "print(sent_equal_0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tenzi_DEnJWc",
        "outputId": "f3daad6f-cc01-464b-aba8-c538bedfff8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length reached in training set: \n",
            "82\n",
            "\n",
            "Number of sentences with length equal to 1 in training set: \n",
            "137\n",
            "\n",
            "Number of sentences with length equal to 0 in training set: \n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "The same type of statistical analysis can be performed also on the validation and test sets. This can be useful to predict in advance, to some extent, whether the methods and techniques adopted for the training phase will work well also in the validation and test phases."
      ],
      "metadata": {
        "id": "SGSRO05fBQUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of sequences in test and validation sets, for statistical purposes\n",
        "\n",
        "print(\"Maximum length in validation set:\")\n",
        "max_len = -1\n",
        "for sent in valid_sents:\n",
        "  if len(sent) > max_len:\n",
        "    max_len = len(sent)\n",
        "print(max_len)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Maximum length in test set:\")\n",
        "max_len = -1\n",
        "for sent in test_sents:\n",
        "  if len(sent) > max_len:\n",
        "    max_len = len(sent)\n",
        "print(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQHtFDkeBOiX",
        "outputId": "e6eee1f9-a797-4e28-918d-82a1a02f6378"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length in validation set:\n",
            "74\n",
            "\n",
            "\n",
            "Maximum length in test set:\n",
            "77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "It may be interesting to plot the histograms that show graphically the frequencies of the various sequence lengths, from the minimal of 0 to the maximal one (defined according to the results previously obtained for the three cases). \n",
        "\n",
        "The obtained graphs, that follow a \"bell curve\" shape, clearly show that in all cases the **most frequent sequence length** corresponds to a value **between 15 and 20**, and **most sentences** are characterized by a length value that is **below 25**."
      ],
      "metadata": {
        "id": "BvHGO7RLZG9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set \n",
        "\n",
        "lengths = list(range(1, 83))\n",
        "frequencies = [0 for i in lengths]\n",
        "for sent in train_sents:\n",
        "  frequencies[len(sent)-1] += 1\n",
        "\n",
        "pos = np.arange(1, 86, 5)\n",
        "width = 0.5  \n",
        "lens = list(range(1, 86, 5))\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xticks(pos + (width / 2))\n",
        "ax.set_xticklabels(lens)\n",
        "ax.set_xlabel(r'Sentence length')\n",
        "ax.set_ylabel(r'Frequence in the training set')\n",
        "\n",
        "plt.bar(lengths, frequencies, width, color='r')\n",
        "plt.show()\n",
        "\n",
        "# Test set\n",
        "\n",
        "lengths = list(range(1, 78))\n",
        "frequencies = [0 for i in lengths]\n",
        "for sent in test_sents:\n",
        "  frequencies[len(sent)-1] += 1\n",
        "\n",
        "pos = np.arange(1, 81, 5)\n",
        "width = 0.5  \n",
        "lens = list(range(1, 81, 5))\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xticks(pos + (width / 2))\n",
        "ax.set_xticklabels(lens)\n",
        "ax.set_xlabel(r'Sentence length')\n",
        "ax.set_ylabel(r'Frequence in the test set')\n",
        "\n",
        "plt.bar(lengths, frequencies, width, color='b')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Validation set \n",
        "\n",
        "lengths = list(range(1, 75))\n",
        "frequencies = [0 for i in lengths]\n",
        "for sent in valid_sents:\n",
        "  frequencies[len(sent)-1] += 1\n",
        "\n",
        "pos = np.arange(1, 76, 5)\n",
        "width = 0.5  \n",
        "lens = list(range(1, 76, 5))\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xticks(pos + (width / 2))\n",
        "ax.set_xticklabels(lens)\n",
        "ax.set_xlabel(r'Sentence length')\n",
        "ax.set_ylabel(r'Frequence in the validation set')\n",
        "\n",
        "plt.bar(lengths, frequencies, width, color='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "jvKy7bNRoaZB",
        "outputId": "230f7e5d-2050-48f8-c663-4451eb750e05"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcZElEQVR4nO3de5RcZZnv8e+PhPstYBomJwETmAAnuLjE5qKAE0ERGSV6hhlgRicwDHGEQRivAVzCnHM4B4cjqKPDECADKAdErgFRDAyXNUsBOyEEknCJAaRjIA0cSBAnMfCcP/bbRdGpy+6q2lV9+X3WqpW939r1vE+lu/qpfXtfRQRmZmYAm3U6ATMzGzpcFMzMrMRFwczMSlwUzMysxEXBzMxKxnY6gWaMHz8+Jk+e3Ok0zMyGlYULF74cEV2VnhvWRWHy5Mn09PR0Og0zs2FF0vPVnvPhIzMzK3FRMDOzEhcFMzMrKawoSJonaY2kJwa0nynpSUlLJf1TWfs5klZIekrSx4rKy8zMqivyRPPVwPeAa/sbJH0YmAnsHxHrJe2S2qcBJwL7Av8FuEfSXhHxVoH5mZnZAIXtKUTEg8CrA5o/D1wUEevTNmtS+0zghohYHxHPAiuAg4vKzczMKmv3OYW9gCMkPSzpAUkHpfaJwAtl2/Wmtk1Imi2pR1JPX19fwemamY0u7S4KY4GdgUOBrwA3StJgAkTE3Ijojojurq6K916YmVmD2l0UeoFbIvMI8DYwHlgF7Fa23aTUZmZmbdTuonAb8GEASXsBWwAvA/OBEyVtKWkKMBV4pM25jWpS9jCz0a2wq48kXQ/MAMZL6gXOB+YB89JlqhuAWZFN/bZU0o3AMmAjcIavPDIzaz8N5+k4u7u7w2MftUb/XsIw/nUws5wkLYyI7krP+Y5mMzMrcVEwM7MSFwUzMytxURihfDWRmTXCRcHMzEpcFMzMrMRFwczMSlwUrCV8DsNsZHBRMDOzEheFUcLf5M0sDxcFMzMrcVEwM7MSFwUzMytxUTAzsxIXBavIJ6bNRicXhWFiuP2RHm75mlnGRcHMzEoKKwqS5klak6beHPjclySFpPFpXZK+K2mFpCWSpheVl5mZVVfknsLVwDEDGyXtBhwN/Kas+ePA1PSYDVxWYF5mZlZFYUUhIh4EXq3w1KXAV4Hy2YBnAtdG5iFgnKQJReVmzfM5A7ORqa3nFCTNBFZFxGMDnpoIvFC23pvaKsWYLalHUk9fX19BmZqZjU5tKwqStgHOBb7RTJyImBsR3RHR3dXV1ZrkzMwMgLFt7GtPYArwmLLjDpOARZIOBlYBu5VtOym1mZlZG7VtTyEiHo+IXSJickRMJjtEND0iXgTmA3+drkI6FHg9Ila3KzczM8sUeUnq9cAvgb0l9Uo6tcbmdwErgRXAFcDpReVlZmbVFXb4KCJOqvP85LLlAM4oKhczM8vHdzSbmVmJi4KZmZW4KJiZWYmLgpmZlbgomJlZiYvCKOWxi8ysEhcFMzMrcVEwM7OSukVB0pQ8bWZmNvzl2VO4uULbTa1OxIY2n4MwGx2qDnMhaR9gX2BHSf+t7KkdgK2KTszMzNqv1thHewOfAMYBnyxrXwecVmRSZmbWGVWLQkTcDtwu6QMR8cs25mRmZh2S55zCK5LulfQEgKT9JH294LzMzKwD8hSFK4BzgD8ARMQS4MQikzIzs87IUxS2iYhHBrRtLCIZG718dZPZ0JCnKLwsaU8gACQdD3iqTDOzEShPUTgDuBzYR9Iq4Gzg8/VeJGmepDX95yJS28WSnpS0RNKtksaVPXeOpBWSnpL0sQbei5mZNaluUYiIlRHxEaAL2CciDo+I53LEvho4ZkDbAuB9EbEf8DTZuQokTSM7T7Fves2/SBqT902YmVlr5Bnm4ixJOwBvApdKWiTp6Hqvi4gHgVcHtP08IvrPRzwETErLM4EbImJ9RDwLrAAOHsT7MDOzFshz+OhvImItcDTwHuCzwEUt6PtvgJ+m5YnAC2XP9aY2MzNrozxFof+akGOBayNiaVlbQySdR3YF03UNvHa2pB5JPX19fc2kYWZmA+QpCgsl/ZysKNwtaXvg7UY7lHQy2fAZfxURkZpXAbuVbTYptW0iIuZGRHdEdHd1dTWahpmZVZCnKJwKzAEOiog3gS2AUxrpTNIxwFeB41KsfvOBEyVtmYblngoMvDfCavB1/mbWCrUGxAMgIt4GFpWtvwK8Uu91kq4HZgDjJfUC55NdbbQlsEDZX7CHIuLvImKppBuBZWSHlc6IiLcG/3bMzKwZdYtCoyLipArNV9XY/kLgwqLyMTOz+jwdpw0LPjxm1h557lP4QZ42s8HwH3mzoSnPnsK+5SvpTuP3F5OOjVQuAmbDQ9WikMYiWgfsJ2lteqwD1gC3ty1DMzNrm6pFISL+d0RsD1wcETukx/YR8Z6IOKeNOZqZWZvkOXx0p6RtASR9RtIlkt5bcF5mZtYBeYrCZcCbkvYHvgT8Gri20KzMzKwj8hSFjWk4ipnA9yLi+8D2xaZlZmadkOfmtXWSziEbHfUISZsBmxeblpmZdUKePYUTgPVkQ2i/SDZY3cWFZmV1+RJPMytCnpnXXgRuJhuzCOBl4NYikzIzs87Ic0fzacBNZPM0Qzb5zW1FJmVmZp2R5/DRGcBhwFqAiHgG2KXIpMzMrDPyFIX1EbGhf0XSWCBqbG9mZsNUnqLwgKRzga0lfRT4MXBHsWmZmVkn5CkKc4A+4HHgc8BdEXFeoVmNQr6ayMyGgjz3KZwZEd8BruhvkHRWajMzsxEkz57CrAptJ7c4DzMzGwJqDZ19kqQ7gCmS5pc97gNerRdY0jxJayQ9Uda2s6QFkp5J/+6U2iXpu5JWSFoiaXor3pyZmQ1OrcNHvwBWA+OBb5W1rwOW5Ih9NfA93j143hzg3oi4SNKctP414OPA1PQ4hGwQvkPyvQUbifrPr4SvczNrq6pFISKeB54HPtBI4Ih4UNLkAc0zgRlp+RrgfrKiMBO4Ng2895CkcZImRMTqRvo2M7PG5Dmn0Eq7lv2hfxHYNS1PBF4o2643tW1C0mxJPZJ6+vr6isu0YL7ayMyGonYXhZK0VzDogwMRMTciuiOiu6urq4DMzMxGr1xFQdLWkvZuQX8vSZqQYk4gm+8ZYBWwW9l2k1KbmZm1UZ4B8T4JLAZ+ltYPkDS/wf7m884lrrOA28va/zpdhXQo8LrPJ5iZtV+ePYULgIOB1wAiYjEwpd6LJF0P/BLYW1KvpFOBi4CPSnoG+EhaB7gLWAmsILtJ7vTBvQ0bbXxOxqwYee5o/kNEvK53fwLrnguIiJOqPHVUhW2DbDRWS3xJppl1Qp6isFTSXwJjJE0FvkB2D4OZmY0weQ4fnQnsSzYl5/Vk8yqcXWRSZmbWGXX3FCLiTeC89DAzsxGsblGQtBfwZWBy+fYRcWRxaZmZWSfkOafwY+BfgSuBt4pNx8zMOilPUdgYEZcVnomZmXVc1aIgaee0eIek04FbyU42AxARdYfPNjOz4aXWnsJCsvsR+m9Q+ErZcwHsUVRSZmbWGbWGzp4CIGmriPjP8uckbVV0YmZm1n557lOodKOab14zMxuBap1T+COyOQ22lnQg7xxG2gHYpg25mZlZm9U6p/Ax4GSyYay/xTtFYS1wbrFpmZlZJ9Q6p3ANcI2kP4uIm9uYk5mZdUjdcwouCGZmo0fHpuM0M7Ohx0XBRgRPumPWGnmGuUDSB9l0QLxrC8rJzMw6JM8oqT8A9iSbp7l/QLwAGi4Kkv4B+NsU53HgFGACcAPwHrK7qT8bERsa7cPMzAYvz55CNzAtTZnZNEkTyWZvmxYRv5d0I3AicCxwaUTcIOlfgVMBD8RnZtZGec4pPAH8UYv7HUt2U9xYshvhVgNHAjel568BPtXiPs3MrI48ewrjgWWSHuHdo6Qe10iHEbFK0v8BfgP8Hvg52eGi1yJiY9qsl+xu6k1Img3MBth9990bScHMzKrIUxQuaGWHknYCZgJTgNfIJvE5Ju/rI2IuMBegu7u7JYe02qH/ypjWHIQzMytGnjmaH2hxnx8Bno2IPgBJtwCHAeMkjU17C5OAVS3u18zM6qh6TkHSf6R/10laW/ZYJ2ltE33+BjhU0jaSBBwFLAPuA45P28wCbm+iDzMza0CtsY8OT/9u38oOI+JhSTcBi4CNwKNkh4N+Atwg6X+mtqta2a+ZmdWX6+a1VouI84HzBzSvBA7uQDpmZpZ4mAszMytxUTAzs5JcRUHSeyV9JC1vLaml5xnMiuYB88zyqVsUJJ1Gdqfx5alpEnBbkUmZmVln5NlTOIPsPoK1ABHxDLBLkUmZmVln5CkK68tHK03jFfm+XDOzEShPUXhA0rlkA9h9lGxYijuKTcvMzDohT1GYA/SRzXvwOeAu4OtFJmVmZp2R5+a1rYF5EXEFgKQxqe3NIhMzM7P2y7OncC9ZEei3NXBPMemYmVkn5SkKW0XEG/0raXmb4lIyM7NOyVMUfidpev+KpPeTTY5jNmT5ZjWzxuQ5p3A28GNJvwVENjXnCYVmZdZhnhTJRqs8k+z8StI+wN6p6amI+EOxaZmZWSfkHTr7IGBy2n66JCLi2sKyGob8zXJ488/PLFO3KEj6AbAnsBh4KzUH4KJgw5aLgFllefYUuoFpEf74mJmNdHmuPnqC7ORyy0gaJ+kmSU9KWi7pA5J2lrRA0jPp351a2aeZmdWXpyiMB5ZJulvS/P5Hk/1+B/hZROwD7A8sJxtO496ImEp2w9ycJvswM7NBynP46IJWdihpR+BDwMkAaQTWDZJmAjPSZtcA9wNfa2Xf7eRj1mY2HNXdU4iIB4DngM3T8q+ARU30OYVsgL1/k/SopCslbQvsGhGr0zYvArtWerGk2ZJ6JPX09fU1kYaZmQ3UyMxrE2lu5rWxwHTgsog4EPgdAw4VpZPaFb9jR8TciOiOiO6urq4m0jAzs4E6MfNaL9AbEQ+n9ZvIisRLkiYApH/XNNGHmZk1oO0zr0XEi8ALkvrvkD4KWAbMB2altlnA7Y32YWZmjclzonngzGun0/zMa2cC10naAlgJnEJWoG6UdCrwPPAXTfZhZmaDlKcozAFO5d0zr13ZTKcRsZjspriBjmomrpmZNSfPgHhvA1ekh5mZjWB5xj56lgrnECJij0IyMjOzjsk79lG/rYA/B3YuJh0zM+ukPDevvVL2WBUR3wb+tA25mZlZm+U5fDS9bHUzsj2HvPMwmJnZMJLnj/u3ypY3kg154ctFzcxGoDxXH324HYmYmVnn5Tl89MVaz0fEJa1Lx8zMOinv1UcHkQ1DAfBJ4BHgmaKSMjOzzshTFCYB0yNiHYCkC4CfRMRnikzMzMzaL8+AeLsCG8rWN1BlroORTHpn4pxK62ZmI0GePYVrgUck3ZrWP0U2M5qZmY0wea4+ulDST4EjUtMpEfFosWmZmVkn5Dl8BLANsDYivgP0SppSYE5mZtYheabjPB/4GnBOatoc+GGRSZmZWWfk2VP4NHAc2VzKRMRvge2LTMpsqPOFBjZS5SkKGyIiSMNnS9q22JQ6w1cXmZnlKwo3SrocGCfpNOAeWjDhjqQxkh6VdGdanyLpYUkrJP0oTdVpZmZtVLMoSBLwI+Am4GZgb+AbEfHPLej7LGB52fo3gUsj4o+B/0c2BaiZmbVRzaKQDhvdFRELIuIrEfHliFjQbKeSJpHNyXBlWhdwJFnxgew+iE8124+ZmQ1OnsNHiyQd1OJ+vw18FXg7rb8HeC0iNqb1XmBipRdKmi2pR1JPX19fi9MyMxvd8hSFQ4CHJP1a0hJJj0ta0miHkj4BrImIhY28PiLmRkR3RHR3dXU1moaZmVVQ9Y5mSbtHxG+Aj7W4z8OA4yQdSzbn8w7Ad8hOZI9NewuTgFUt7tfMzOqotadwG0BEPA9cEhHPlz8a7TAizomISRExGTgR+PeI+CvgPuD4tNks4PZG+zAzs8bUKgrlV+nvUXQiZHdNf1HSCrJzDFe1oU8zMytTa0C8qLLcMhFxP3B/Wl4JHFxEP2Zmlk+torC/pLVkewxbp2XSekTEDoVnZ2ZmbVW1KETEmHYmYjaU9Q95EoXsM5sNHXmHzjYzs1HARcHMzEpcFMxawKPq2kjhomBmZiUuCmZmVuKiYFYAH06y4cpFwczMSlwUzMysxEXBzMxKXBTM2sDnGGy4cFEwM7MSFwUzMytxUTAzsxIXBbMO8DkGG6pcFMzMrKTtRUHSbpLuk7RM0lJJZ6X2nSUtkPRM+nendudmZjbadWJPYSPwpYiYBhwKnCFpGjAHuDcipgL3pnUzM2ujtheFiFgdEYvS8jpgOTARmAlckza7BvhUu3MzMxvtOnpOQdJk4EDgYWDXiFidnnoR2LVDaZmZjVodKwqStgNuBs6OiLXlz0VEABVnw5U0W1KPpJ6+vr42ZGpmNnp0pChI2pysIFwXEbek5pckTUjPTwDWVHptRMyNiO6I6O7q6mpPwmZmo0Qnrj4ScBWwPCIuKXtqPjArLc8Cbm93bmad4vsWbKgY24E+DwM+CzwuaXFqOxe4CLhR0qnA88BfdCA3M7NRre1FISL+A6j2neioduZiNlT17zVExTNrZsXxHc1mZlbiomBmZiUuCmZD3MCT0D4pbUVyUTAzsxIXBTMzK3FRMDOzEhcFMzMrcVEwM7MSFwUzMytxUTAzsxIXBTOryfdFjC4uCmZmVuKiYDbK+Ju/1eKiYDbM1RsGw0XABmPUFgV/UMwy/ixYuVFbFMzMbFMuCmbWFO9pjCwuCmZmVjLkioKkYyQ9JWmFpDmdzsdstKn3zb/ZPYPBvt57Iu01pIqCpDHA94GPA9OAkyRN62xWZlakoq+WGmlFpej3M6SKAnAwsCIiVkbEBuAGYGaHczIzGzXGdjqBASYCL5St9wKHlG8gaTYwO62+IempZjqUGA+8XLY+8PnBrjue4xUSbyjnVmm9gpbmNzBePYPNrwUKjdfk3sJ7qz0x1IpCXRExF5jbqniSeiKi2/Ecb7jHG8q5Od7Qi1fNUDt8tArYrWx9UmozM7M2GGpF4VfAVElTJG0BnAjM73BOZmajxpA6fBQRGyX9PXA3MAaYFxFLC+62ZYeiHM/xOhxvKOfmeEMvXkWKiHb0Y2Zmw8BQO3xkZmYd5KJgZmYlo7YoSJonaY2kJ1oYc5ykmyQ9KWm5pA80m5OkP5e0VNLbkgZ1OVq19yjpzJTjUkn/lDPWbpLuk7Qsve6sZvKrFq+J/LaS9Iikx9Lr/jG1/30aMiUkjR9EftXiSdKFkp5OP+Mv5I2ZXj9G0qOS7mwmvxrxGs5P0nOSHpe0WFJPamvm92+TeKm9kZ/vJp+tJnOr+FltMLe903vsf6yVdHYTn42K8RrNb9AiYlQ+gA8B04EnWhjzGuBv0/IWwLhmcwL+K7A3cD/Q3YJ4HwbuAbZM67vkjDUBmJ6WtweeJhuKpKH8asRrND8B26XlzYGHgUOBA4HJwHPA+EHkVy3eKcC1wGaDya8s7heB/wvcmdYbyq9GvIbzq5RDk79/leI1+vPd5LPVZG6V4jWU24C4Y4AXyW4Oazi/KvGazi/PY0hdfdROEfGgpMmtiidpR7I/wien+BuADc3mFBHLU/xB51TlPX4euCgi1qdt1uSMtRpYnZbXSVoOTIyIBY3kVy0ecFqD+QXwRlrdPD0iIh5tML+K8cj+//4yIt4eTH4ph0nAnwIXkv0xp9H8qsVrJr9Kmvn9q2LQv381PluvNZJbtXiSGvpsDHAU8OuIeL6svwbCbBpP0sUtyK+uUXv4qABTgD7g39Lu/JWStu10UhXsBRwh6WFJD0g6aLABUqE5kOzbc9MGxGs4v3QoZTGwBlgQEU3lVyXensAJknok/VTS1EGE/DbwVeDtZvKqE6+Z/AL4uaSFyoaTaValeI38fFv92aoWr+nPBtm9Vdc3kVuteK3Iry4XhdYZS3ao5rKIOBD4HTAUh/4eC+xMdijkK8CNGsRXGUnbATcDZ0fE2maTqRCv4fwi4q2IOIDsTviDJb2vmdyqxNsS+M/Ihhu4ApiXJ5akTwBrImJhMznliNdQfsnhETGdbJTiMyR9qMk0K8Vr5Ofb6s9WtXjNfja2AI4DftxEbrXiNZVfXi4KrdML9JZ9O72J7BdvqOkFbonMI2TfMnOd4JS0Odkf8Osi4pZmE6kSr+H8+kXEa8B9wDHN5lghXi/Qn+utwH45wxwGHCfpObLRf4+U9MMm0qoWr9H8iIhV6d816bUHN5FftXiN/Hxb/dmqFq/Z372PA4si4qUmcqsVr+nPRh4uCi0SES8CL0jaOzUdBSzrYErV3EZ2wgpJe5GdZKs7kmP6RnIVsDwiLmk2iRrxGs2vS9K4tLw18FHgySbyqxavlB/wJ2QnyOuKiHMiYlJETCY7JPDvEfGZRvOrEa+h/CRtK2n7/mXgaKDhK/NqxBv0z7fVn60a8Rr63StzEq09dDQwXrP55VPE2evh8Ej/2auBP5BV4FNbEPMAoAdYkn6AOzWbE/DptLweeAm4u8l4WwA/JPuALgKOzBnrcLJjxEuAxelxbKP51YjXaH77AY+meE8A30jtX0j5bQR+C1zZZLxxwE+Ax4FfAvs38Hsyg3euFmoovxrxGsoP2AN4LD2WAuel9kZ/vtXiNfrz3eSz1eRno1K8hnJL8bYFXgF2LGtrJr9K8RrObzAPD3NhZmYlPnxkZmYlLgpmZlbiomBmZiUuCmZmVuKiYGZmJS4KNmxJOi+NFrkkjSZ5SINxDpB0bKvzy9n3ZLVwpN6yuDMkfbBs/WpJx7e6Hxt5Ru2AeDa8KRvq+BNkI62uVzbs9BYNhjsA6AbualV+Q8AMsgH9ftHhPGyY8Z6CDVcTgJfjnREjX46I3wJIen8aMGyhpLslTUjt90v6prJ5Ep6WdEQaX+a/kw0it1jSCelu3Hlpu0clzUyvP1nSLZJ+JukZlY1nL+kYSYuUzb9wb2qrGKeaNADfxZJ+lfZ+PpfaZ6Tc+8f/v65/zBtJx6a2hZK+K+lOZQMM/h3wD+k9HZG6+JCkX0ha6b0Gq6qIO+L88KPoB7Ad2V3QTwP/AvxJat+c7NtxV1o/AZiXlu8HvpWWjwXuScsnA98ri/2/gM+k5XGpj23TdiuBHYGtgOeB3YAu4AVgSnrNzrXiDHgfk0nzXQCzga+n5S3J7ridQvat/3Wygfk2I7tT+fCUQ3m/1/POnc0XAF8u6+dqsoHVNiObt2JFp3+GfgzNhw8f2bAUEW9Iej9wBNl4MD+SNIfsD+n7gAXpy/QY0rwNSf9gcQvJ/iBXcjTZYHNfTutbAbun5Xsj4nUAScvIJj/ZCXgwIp5Nub1aJ87yGv3uV/YtfkdgKtncAY9ERG/qd3HK/Q1gZX+/ZEWh1pDXt0U2z8IySbvW2M5GMRcFG7Yi4i2yb//3S3ocmEX2x35pRFSbCnV9+vctqv/+C/iziHjqXY3Ziez1ZU21YlSNU2f7MyPi7gH9zhhkv9WUx2j5kMs2Mvicgg1LyuaxLZ9A5gCywzlPAV16Z87dzSXtWyfcOrIpQfvdDZxZdtz+wDqvf4jseP2UtP3ODca5G/h8GlIcSXup9mQyTwF76J3Z9U6o8Z7McnFRsOFqO+AaScskLSE7Tn5BZFMrHg98U9JjZOcdPlgjDmRzJUzrP9EM/A+ycxNLJC1N61VFRB/ZYZtbUp8/Sk8NKg5wJdkQzovSZaqXU2OPICJ+D5wO/EzSQrJC8Hp6+g7g0wNONJvV5VFSzYYxSdul8ysCvg88ExGXdjovG768p2A2vJ2WTjwvJTsxfXmH87FhznsKZmZW4j0FMzMrcVEwM7MSFwUzMytxUTAzsxIXBTMzK/n/qBosiwnqHFsAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "A fundamental value that must be taken into account for the definition of the capacity (and hence the number of parameters/layers/units of the model) is the **dimension of the training set**. The more samples are contained in the training set, the easiest it is to avoid overfitting (with properly adopted regularization methods), while maintaining a fixed number of parameters in the architectural framework. "
      ],
      "metadata": {
        "id": "-f-95P4GRhV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of samples in the training set\n",
        "print(\"Train: \", len(train_sents), \"\\n\")\n",
        "\n",
        "# Number of samples in the test set\n",
        "print(\"Test: \", len(test_sents), \"\\n\")\n",
        "\n",
        "# Number of samples in the validation set\n",
        "print(\"Valid: \", len(valid_sents), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8gbiU47SnW0",
        "outputId": "12ca2213-9820-4dfe-cc31-c9fb616a75fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  42068 \n",
            "\n",
            "Test:  3761 \n",
            "\n",
            "Valid:  3370 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "\n",
        "Additionally to the aforementioned points, it's important to notice that **there are no OOV words** in the test and validation sets, that is, all the words belonging to the test and validation sets already belong to the training set. \n",
        "\n",
        "This implies that, in order to define a reference vocabulary, it suffices to **consider just the words contained in the training set**.\n",
        "\n",
        "The total number of words contained in the training set is **9999**."
      ],
      "metadata": {
        "id": "e7NVQAITKPCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test words set is contained in training words set: \") \n",
        "print(test_words.issubset(train_words), \"\\n\")\n",
        "\n",
        "print(\"Validation words set is contained in training words set : \")\n",
        "print(valid_words.issubset(train_words), \"\\n\")\n",
        "\n",
        "# Number of words in the training set\n",
        "print(\"Total number of words in the training set: \", len(train_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HTcRgNGKwau",
        "outputId": "eb8592b0-283b-4893-a20f-d0e2ecaa5564"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test words set is contained in training words set: \n",
            "True \n",
            "\n",
            "Validation words set is contained in training words set : \n",
            "True \n",
            "\n",
            "Total number of words in the training set:  9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "The **50 most frequent words** for the tree sets of the dataset are now computed. \n",
        "\n",
        "It is possible to notice that, along with the most common prepositions, such as \"the\" and \"of\", the special tokens **\"N\"** and \"**\\<unk\\>**\" correspond to the words that appear most frequently."
      ],
      "metadata": {
        "id": "gVI-igNAHBPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set \n",
        "\n",
        "training_freqdist = nltk.FreqDist(train_words_list)\n",
        "print(\"First 50 words of the training Frequency Dictionary: \")\n",
        "print(dict(sorted(training_freqdist.items(), key=lambda item: item[1], reverse=True)[:50]), '\\n')\n",
        "\n",
        "# Test set \n",
        "\n",
        "test_freqdist = nltk.FreqDist(test_words_list)\n",
        "print(\"First 50 words of the test Frequency Dictionary: \")\n",
        "print(dict(sorted(test_freqdist.items(), key=lambda item: item[1], reverse=True)[:50]), '\\n')\n",
        "\n",
        "# Validation set \n",
        "\n",
        "valid_freqdist = nltk.FreqDist(valid_words_list)\n",
        "print(\"First 50 words of the validation Frequency Dictionary: \")\n",
        "print(dict(sorted(valid_freqdist.items(), key=lambda item: item[1], reverse=True)[:50]), '\\n')"
      ],
      "metadata": {
        "id": "1uidO7oxHAPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdef6727-a662-4b37-c790-0f9703484414"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 50 words of the training Frequency Dictionary: \n",
            "{'the': 50770, '<unk>': 45020, 'N': 32481, 'of': 24400, 'to': 23638, 'a': 21196, 'in': 18000, 'and': 17474, \"'s\": 9784, 'that': 8931, 'for': 8927, '$': 7541, 'is': 7337, 'it': 6112, 'said': 6027, 'on': 5650, 'by': 4915, 'at': 4894, 'as': 4833, 'from': 4724, 'million': 4627, 'with': 4585, 'mr.': 4326, 'was': 4073, 'be': 3923, 'are': 3914, 'its': 3846, 'he': 3632, 'but': 3541, 'has': 3494, 'an': 3477, \"n't\": 3388, 'will': 3270, 'have': 3245, 'new': 2793, 'or': 2704, 'company': 2680, 'they': 2562, 'this': 2438, 'year': 2379, 'which': 2362, 'would': 2308, 'about': 2220, 'says': 2092, 'more': 2065, 'were': 2009, 'market': 2005, 'billion': 1881, 'his': 1852, 'had': 1850} \n",
            "\n",
            "First 50 words of the test Frequency Dictionary: \n",
            "{'<unk>': 4794, 'the': 4529, 'N': 2523, 'of': 2195, 'to': 2042, 'a': 1821, 'in': 1640, 'and': 1539, \"'s\": 903, 'that': 831, 'for': 783, 'is': 667, 'said': 601, '$': 564, 'it': 542, 'on': 507, 'as': 461, 'by': 450, 'at': 420, 'with': 405, 'its': 393, 'was': 391, 'be': 384, 'are': 365, 'from': 355, 'million': 348, \"n't\": 335, 'but': 335, 'mr.': 319, 'have': 309, 'he': 302, 'market': 300, 'has': 296, 'will': 294, 'an': 260, 'about': 258, 'this': 255, 'company': 227, 'or': 220, 'they': 220, 'new': 219, 'year': 214, 'were': 203, 'which': 202, 'more': 198, 'would': 189, 'u.s.': 179, 'stock': 175, 'had': 175, 'than': 173} \n",
            "\n",
            "First 50 words of the validation Frequency Dictionary: \n",
            "{'the': 4122, '<unk>': 3485, 'N': 2603, 'of': 1832, 'to': 1750, 'a': 1738, 'in': 1392, 'and': 1391, \"'s\": 868, 'for': 726, '$': 659, 'that': 657, 'it': 537, 'is': 529, 'said': 513, 'on': 486, 'at': 453, 'was': 436, 'as': 402, 'with': 364, 'from': 356, 'million': 356, 'market': 342, 'by': 340, 'its': 331, 'but': 316, 'mr.': 304, \"n't\": 283, 'he': 283, 'be': 279, 'an': 257, 'has': 250, 'are': 248, 'or': 245, 'new': 239, 'have': 232, 'says': 217, 'stock': 216, 'will': 202, 'were': 202, 'would': 198, 'which': 192, 'had': 192, 'about': 190, 'they': 180, 'company': 170, 'up': 163, 'more': 160, 'year': 160, 'trading': 159} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Definition of the vocabulary\n",
        "\n",
        "The vocabulary is defined as an instance of the class **Vocab**, and it is composed by two dictionaries that map each word from the dataset to a particular index and viceversa.\n",
        "\n",
        "As mentioned before, it suffices to consider the words contained in the training set, since it contains also all the words from the test and validation sets. Additionally to these ones, the dictionary also contains two special tokens:\n",
        "\n",
        "*  **'pad'**, that is the padding token (which is defined as \"0\"), that will be useful for constructing the batches in the following steps;\n",
        "*  **'\\</s\\>'**, the end-of-sentence token, which is added at the end of each sentence of the datasets before the construction of the batches. \n"
      ],
      "metadata": {
        "id": "o5gEzpPGkn3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab():\n",
        "\n",
        "    def __init__(self, words):\n",
        "        self.word2id = self.w2id(words)\n",
        "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
        "        \n",
        "    def w2id(self, elements):\n",
        "        vocab = {'pad': PAD_TOKEN, '</s>': 1}\n",
        "        for w in list(elements):\n",
        "          if w not in vocab:\n",
        "            vocab[w] = len(vocab)\n",
        "        return vocab\n",
        "\n",
        "\n",
        "# Creation of the vocabulary containing all the words in training, test and validation sets\n",
        "vocab = Vocab(train_words_list)"
      ],
      "metadata": {
        "id": "6eY7bU4Vkq4z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "The dimension of the two dictionaries is hence 9999 + 1 + 1 = **10001**."
      ],
      "metadata": {
        "id": "KyHxIzKxSrHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocab.id2word))\n",
        "print(len(vocab.word2id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id_xtdtVFqAa",
        "outputId": "48d229d9-d5e8-43b6-b5cf-34a73ca7d846"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10001\n",
            "10001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "## <center>First baseline: N-gram statistical language models</center>\n",
        "\n",
        "In order to better evaluate the results that will be obtained through the Neural Language Model, a statistical baseline is implemented through three **n-gram Language Models** and considered as a starting point reference."
      ],
      "metadata": {
        "id": "xKizraQYygRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparation of test data \n",
        "\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "\n",
        "test_sentences_padding_2 = [w for sent in test_sents for w in list(pad_both_ends(sent, n=2))]\n",
        "test_sentences_padding_3 = [w for sent in test_sents for w in list(pad_both_ends(sent, n=3))]\n",
        "\n",
        "test_ngrams_padding = {}\n",
        "test_ngrams_padding[2] = list(nltk.bigrams(test_sentences_padding_2))\n",
        "test_ngrams_padding[3] = list(nltk.trigrams(test_sentences_padding_3))"
      ],
      "metadata": {
        "id": "s8YfYVH6ymEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and evaluation of 2-gram and 3-gram Language Models:\n",
        "\n",
        "* MLE\n",
        "* Laplace\n",
        "* Lidstone\n",
        "\n",
        "It is possible to verify that the result are not outstanding, and the **bigram** Language Models (~ 900 in test perplexity) behave better with respect to the **trigram** Language Models (~ 3000 in test perplexity) in this setting."
      ],
      "metadata": {
        "id": "o5N5QzTi_hRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "from nltk.lm import MLE, Laplace, Lidstone\n",
        "\n",
        "print(\"MLE: \\n\")\n",
        "\n",
        "for ngram_size in [2,3]:\n",
        "\n",
        "    print(\"n-gram size: \", ngram_size, \"\\n\")\n",
        "    \n",
        "    # Preparation of training data\n",
        "    padded_ngrams, flat_text = list(padded_everygram_pipeline(ngram_size, train_sents))\n",
        "    \n",
        "    mle_lm = MLE(ngram_size)\n",
        "    mle_lm.fit(padded_ngrams, flat_text)\n",
        "    print(\"MLE ngram counts: \", mle_lm.counts)\n",
        "    print(\"MLE entropy: \", mle_lm.entropy(test_ngrams_padding[ngram_size]))\n",
        "    print(\"MLE perplexity: \", mle_lm.perplexity(test_ngrams_padding[ngram_size]))\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "print(\"Laplace: \\n\")\n",
        "\n",
        "for ngram_size in [2,3]:\n",
        "\n",
        "    print(\"n-gram size: \", ngram_size, \"\\n\")\n",
        "    \n",
        "    padded_ngrams, flat_text = list(padded_everygram_pipeline(ngram_size, train_sents))\n",
        "    \n",
        "    # 1) Laplace\n",
        "    laplace_lm = Laplace(ngram_size)\n",
        "    laplace_lm.fit(padded_ngrams, flat_text)\n",
        "    print(\"Laplace ngram counts: \", laplace_lm.counts)\n",
        "    print(\"Laplace entropy: \", laplace_lm.entropy(test_ngrams_padding[ngram_size]))\n",
        "    print(\"Laplace perplexity: \", laplace_lm.perplexity(test_ngrams_padding[ngram_size]))\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "print(\"Lidstone: \\n\")\n",
        "\n",
        "for ngram_size in [2,3]:\n",
        "\n",
        "    print(\"n-gram size: \", ngram_size, \"\\n\")\n",
        "\n",
        "    padded_ngrams, flat_text = list(padded_everygram_pipeline(ngram_size, train_sents))\n",
        "    \n",
        "    # 2) Lidstone\n",
        "    # also requires gamma parameter\n",
        "    lidstone_lm = Lidstone(order=ngram_size, gamma=1)\n",
        "    lidstone_lm.fit(padded_ngrams, flat_text)\n",
        "    print(\"Lidstone ngram counts: \", lidstone_lm.counts)\n",
        "    print(\"Lidstone entropy: \", lidstone_lm.entropy(test_ngrams_padding[ngram_size]))\n",
        "    print(\"Lidstone perplexity: \", lidstone_lm.perplexity(test_ngrams_padding[ngram_size]))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFAVwOtB18Na",
        "outputId": "0605e389-43e9-410d-afbe-2036b075b5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLE: \n",
            "\n",
            "n-gram size:  2 \n",
            "\n",
            "MLE ngram counts:  <NgramCounter with 2 ngram orders and 1901246 ngrams>\n",
            "MLE entropy:  inf\n",
            "MLE perplexity:  inf\n",
            "\n",
            "\n",
            "n-gram size:  3 \n",
            "\n",
            "MLE ngram counts:  <NgramCounter with 3 ngram orders and 3041175 ngrams>\n",
            "MLE entropy:  inf\n",
            "MLE perplexity:  inf\n",
            "\n",
            "\n",
            "Laplace: \n",
            "\n",
            "n-gram size:  2 \n",
            "\n",
            "Laplace ngram counts:  <NgramCounter with 2 ngram orders and 1901246 ngrams>\n",
            "Laplace entropy:  9.843858866753022\n",
            "Laplace perplexity:  918.9603891637692\n",
            "\n",
            "\n",
            "n-gram size:  3 \n",
            "\n",
            "Laplace ngram counts:  <NgramCounter with 3 ngram orders and 3041175 ngrams>\n",
            "Laplace entropy:  11.667129359774286\n",
            "Laplace perplexity:  3252.0401633846063\n",
            "\n",
            "\n",
            "Lidstone: \n",
            "\n",
            "n-gram size:  2 \n",
            "\n",
            "Lidstone ngram counts:  <NgramCounter with 2 ngram orders and 1901246 ngrams>\n",
            "Lidstone entropy:  9.843858866753022\n",
            "Lidstone perplexity:  918.9603891637692\n",
            "\n",
            "\n",
            "n-gram size:  3 \n",
            "\n",
            "Lidstone ngram counts:  <NgramCounter with 3 ngram orders and 3041175 ngrams>\n",
            "Lidstone entropy:  11.667129359774286\n",
            "Lidstone perplexity:  3252.0401633846063\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "## <center>Dataset and Dataloader</center>\n",
        "\n",
        "The code used for the definition of the datasets and dataloaders was inspired and adapted from the *10_sequence_nn.ipynb* Notebook explained during the lectures of the course."
      ],
      "metadata": {
        "id": "_j1FMJh4a8ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function that returns all the sentences in the txt from the input path, \n",
        "with the addition of the end-of-sequence token </s>.\n",
        "\n",
        ":param path: path to the txt dataset\n",
        ":return sents: list of all sentences, each one ending with </s>\n",
        "\"\"\"\n",
        "def list_sents_eos(path):\n",
        "    sents = []\n",
        "    with open(path) as file:\n",
        "        for line in file:\n",
        "          sent = line.split()\n",
        "          sent.insert(len(sent), '</s>')\n",
        "          sents.append(sent)\n",
        "    file.close()\n",
        "    return sents\n",
        "\n",
        "# Sentences in the training set, with the end-of-sentence token </s>\n",
        "train_data = list_sents_eos(\"gdrive/MyDrive/NLU_project/dataset/ptb.train.txt\")\n",
        "\n",
        "# Sentences in the test set, with the end-of-sentence token </s>\n",
        "test_data = list_sents_eos(\"gdrive/MyDrive/NLU_project/dataset/ptb.test.txt\")\n",
        "\n",
        "# Sentences in the validation set, with the end-of-sentence token </s>\n",
        "valid_data = list_sents_eos(\"gdrive/MyDrive/NLU_project/dataset/ptb.valid.txt\")"
      ],
      "metadata": {
        "id": "kaM9yuvoERUq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Definition of the datasets\n",
        "\n",
        "The datasets are defined as instances of the class **SentsDataset**, and their elements are tensors, each one corresponding to a particular sentence, containing the numerical indexes to which each original word is mapped through the **vocab** dictionary maps."
      ],
      "metadata": {
        "id": "HuQi0YtUI3yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentsDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, dataset, vocab):\n",
        "        self.sentences = dataset\n",
        "        self.sents_ids = self.mapping_seq(self.sentences, vocab.word2id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = torch.Tensor(self.sents_ids[idx])\n",
        "        return sent\n",
        "    \n",
        "    # Auxiliary method\n",
        "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
        "        res = []\n",
        "        for seq in data:\n",
        "            tmp_seq = []\n",
        "            for x in seq:\n",
        "                tmp_seq.append(mapper[x])\n",
        "            res.append(tmp_seq)\n",
        "        return res\n",
        "\n",
        "\n",
        "# Training dataset\n",
        "train_dataset = SentsDataset(train_data, vocab)\n",
        "\n",
        "# Validation dataset\n",
        "valid_dataset = SentsDataset(valid_data, vocab)\n",
        "\n",
        "# Test dataset\n",
        "test_dataset = SentsDataset(test_data, vocab)"
      ],
      "metadata": {
        "id": "HAYf3prdM0kK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Definition of the dataloaders\n",
        "\n",
        "The **Dataloader** class is needed in order to split the data into batches and add padding. Padding is a strategy used to fit sequences of different lengths into a matrix.\n",
        " \n",
        "```python\n",
        "DataLoader(Dataset, batch_size=N, collate_fn={custom function}, shuffle=True)\n",
        "```\n",
        "\n",
        "The Dataloader provides batches containing all the same number of sentences, and the batch size is fixed when the Dataloader is first defined. For what concerns the second dimension, it can vary from batch to batch and it corresponds to the **length of the longest sentence in the batch** itself. \n",
        "The sentences within a single batch are ordered according to their length, from the longest one to the shortest, and padding is performed through the **'pad'** token on the right of the sentences whose length is lower than **max_len**.\n",
        "\n",
        "In order to customize the shape and composition of the output, an appropriate *collate_fn* function is defined. "
      ],
      "metadata": {
        "id": "z5hSouir7Yrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(sents):\n",
        "\n",
        "    def merge(sequences):\n",
        "        \"\"\"\n",
        "        Function used to merge from batch * sent_len to batch * max_len\n",
        "        \"\"\"\n",
        "        lengths = [len(seq) for seq in sequences]\n",
        "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
        "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
        "        # The matrix is filled with PAD_TOKENS \n",
        "        # And then each sentence is copied into the matrix\n",
        "        for i, seq in enumerate(sequences):\n",
        "            end = lengths[i]\n",
        "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
        "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
        "\n",
        "        return padded_seqs, lengths\n",
        "\n",
        "    # Sort data by seq lengths\n",
        "    sents.sort(key=lambda s: len(s), reverse=True) \n",
        "\n",
        "    # Input sequences\n",
        "    sents_in = [s[:-1] for s in sents]\n",
        "    # Output (target) sequences\n",
        "    sents_out = [s[1:] for s in sents]\n",
        "\n",
        "    # The sequence length is the same in both cases\n",
        "    padded_in, seq_lengths = merge(sents_in)\n",
        "    padded_out, _ = merge(sents_out)\n",
        "\n",
        "    # Length of input/output\n",
        "    seq_lengths = torch.LongTensor(seq_lengths).to(device)\n",
        "\n",
        "    return (padded_in, padded_out, seq_lengths)"
      ],
      "metadata": {
        "id": "obbF5mNQ7ePW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **batch_size** is set to **64** for both the training and evaluation phases. This value has shown to work well empirically for the considered task: it allows for a both fast and stable training procedure."
      ],
      "metadata": {
        "id": "uNLbAGoUfuTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "# Training dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn,  shuffle=True)\n",
        "\n",
        "# Validation dataloader\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# Test dataloader\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "7GhqP_EtfpfX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "## <center>Baseline Neural Models</center>"
      ],
      "metadata": {
        "id": "G65lBDgilXrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Definition of the baseline models\n",
        "\n",
        "Two baseline **Neural Language Models** are defined and evaluated: a simple RNN architecture with a Vanilla RNN cell, and the same architecture with an LSTM cell in substitution of the Vanilla cell. \n",
        "The LSTM provides some improvements in the training phase with respect to the simple Vanilla cell, since it allows to significantly reduce the issue of **vanishing and exploding gradients**, that make the training slower and more difficult. \n",
        "\n",
        "They are composed of the following modules:\n",
        "\n",
        "* **Simple RNN**: \n",
        "    -   embedding layer \n",
        "    -   vanilla RNN cell \n",
        "    -   linear decoder layer \n",
        "* **LSTM**:  \n",
        "    -   embedding layer \n",
        "    -   LSTM cell \n",
        "    -   linear decoder layer \n"
      ],
      "metadata": {
        "id": "Ht6Lf3ZniW43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Baseline_NLM(nn.Module):\n",
        "\n",
        "    def __init__(self, hid_size, emb_size, vocab_len, lstm = False, n_layer = 1, pad_index = 0):\n",
        "\n",
        "        super(Baseline_NLM, self).__init__()  \n",
        "\n",
        "        # Word embedding \n",
        "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
        "        \n",
        "        if lstm:\n",
        "          # We set bidirectionality to false, so the LSTM is one-directional \n",
        "          self.rnn = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=False) \n",
        "        else:\n",
        "          self.rnn = nn.RNN(emb_size, hid_size, n_layer)   \n",
        "\n",
        "        # Output layer\n",
        "        self.decoder = nn.Linear(hid_size, vocab_len)\n",
        "        \n",
        "    def forward(self, sentences, seq_lengths):\n",
        "\n",
        "        # sentences: (batch_size, seq_len)\n",
        "        sents_emb = self.embedding(sentences) \n",
        "        # sents_emb: (batch_size, seq_len, emb_size)\n",
        "\n",
        "        # Swap the matrix in order to have dimensions setted in another way:\n",
        "        # We need seq len first -> permute\n",
        "        sents_emb = sents_emb.permute(1,0,2) \n",
        "        # sents_emb: (seq_len, batch_size, emb_size)\n",
        "        \n",
        "        # Compression\n",
        "        # pack_padded_sequence avoids computation over pad tokens reducing the computational cost\n",
        "        packed_input = pack_padded_sequence(sents_emb, seq_lengths.cpu().numpy())\n",
        "\n",
        "        # Process the batch through the RNN layer\n",
        "        packed_output, _ = self.rnn(packed_input) \n",
        "\n",
        "        # Unpack the sequence: decompression\n",
        "        padded_output, _ = pad_packed_sequence(packed_output)\n",
        "\n",
        "        # Get the output\n",
        "        out = self.decoder(padded_output)\n",
        "\n",
        "        # Swap the output in order to have dimensions setted correctly for the loss\n",
        "        out = out.permute(1,0,2).contiguous()\n",
        "        out = out.view(-1, out.shape[-1])\n",
        "        # out: (batch_size * seq_len, vocab_len)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Uk_-uVbBnIx2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Function to randomly initialize the weights of the architecture\n",
        "\n",
        "Weight initialization is an essential procedure that allows to stabilize the training phase. "
      ],
      "metadata": {
        "id": "xMUIEPcTs18n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(mat):\n",
        "    for m in mat.modules():\n",
        "        if type(m) in [nn.LSTM, nn.RNN]:\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight_ih' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
        "                elif 'weight_hh' in name:\n",
        "                    for idx in range(4):\n",
        "                        mul = param.shape[0]//4\n",
        "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
        "                elif 'bias' in name:\n",
        "                    param.data.fill_(0.00)\n",
        "        else:\n",
        "            if type(m) in [nn.Linear]:\n",
        "                torch.nn.init.uniform_(m.weight, -0.1, 0.1)\n",
        "                if m.bias != None:\n",
        "                    m.bias.data.fill_(0.01)\n",
        "            if type(m) in [nn.Embedding]:\n",
        "                torch.nn.init.uniform_(m.weight, -0.1, 0.1)"
      ],
      "metadata": {
        "id": "18uY9IuksyZr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Training loop"
      ],
      "metadata": {
        "id": "qPsZBhhUznD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop_baseline(data_loader, optimizer, cost_function, model, device = 'cuda:0'):\n",
        "\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Cumulative loss values obtained for each batch\n",
        "    loss_sum_array = []\n",
        "\n",
        "    # Total number of words in the batch\n",
        "    num_words_array = []\n",
        "\n",
        "    # Iterate over the training set\n",
        "    for batch_idx, (inputs, targets, lengths) in enumerate(data_loader):\n",
        "\n",
        "        # Load data into GPU\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "          \n",
        "        # Gradients reset\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, lengths)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = cost_function(outputs,targets.view(-1))\n",
        "\n",
        "        # Update of the loss array and number of words array\n",
        "        loss_sum_array.append(loss.item())\n",
        "        num_words_array.append((torch.sum(lengths)).item())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Parameters update\n",
        "        optimizer.step()\n",
        "      \n",
        "    return loss_sum_array, num_words_array"
      ],
      "metadata": {
        "id": "CaVFATU0zwcQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Evaluation loop"
      ],
      "metadata": {
        "id": "Tima3RyW7cjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_loop_baseline(data_loader, cost_function, model, device = 'cuda:0'):\n",
        "\n",
        "  # Set the network to evaluation mode\n",
        "  model.eval() \n",
        "\n",
        "  # Cumulative loss values obtained for each batch\n",
        "  loss_sum_array = []\n",
        "\n",
        "  # Total number of words in the batch\n",
        "  num_words_array = []\n",
        "\n",
        "  # Disable gradient computation \n",
        "  with torch.no_grad():\n",
        "\n",
        "    # Iterate over the test set\n",
        "    for batch_idx, (inputs, targets, lengths) in enumerate(data_loader):\n",
        "      \n",
        "        # Load data into GPU\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "          \n",
        "        # Forward pass\n",
        "        outputs = model(inputs, lengths)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = cost_function(outputs, targets.view(-1))\n",
        "\n",
        "        # Update of the loss array\n",
        "        loss_sum_array.append(loss.item())\n",
        "        num_words_array.append((torch.sum(lengths)).item())\n",
        "      \n",
        "  return loss_sum_array, num_words_array"
      ],
      "metadata": {
        "id": "6gOlqAY07kUv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Definition of the training procedure\n",
        "\n",
        "The neural networks are trained by iterating several times (that is, for several **epochs**) over the training set.\n",
        "\n",
        "The number of **epochs** defines the number of times the whole training set is seen by the network, and this quantity was set to a maximum fixed value of **200**. Indeed, it is most likely that less than 200 epochs are performed in practice, since an **early stopping** procedure is applied. \n",
        "\n",
        "**Early stopping** is a regularization technique that allows to save the network and stop the training procedure before the models overfits too much and the generalization gap starts being excessive. It is put in place by monitoring the **validation loss** (that is the loss computed on the validation set) every few steps, and stopping the training procedure when such loss is not subject to any improvement for a number of iterations equal to a certain fixed value called **patience**. So this value tells how many steps of not improving performance one has to wait before stopping the training. \n",
        "\n",
        "A **patience** value that has shown to work well in practice is **3**.\n",
        "In order to put in place the early stopping procedure, the **validation set** is used."
      ],
      "metadata": {
        "id": "tCiam5tbj7wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_baseline(baseline_model,\n",
        "         prediction_loss, \n",
        "         optimizer, \n",
        "         train_loader, \n",
        "         valid_loader, \n",
        "         test_loader,\n",
        "         n_epochs = 200,   \n",
        "         patience = 3):\n",
        "\n",
        "    # Vectors filled during training that allow to plot the loss variations across epochs\n",
        "    losses_train_baseline = []\n",
        "    losses_valid_baseline = []\n",
        "    sampled_epochs_baseline = []\n",
        "\n",
        "    # Initial best loss value, used for the early stopping technique\n",
        "    best_loss = 100.00\n",
        "\n",
        "\n",
        "    for x in tqdm(range(n_epochs)):\n",
        "\n",
        "        # For each epoch:\n",
        "        print('Epoch n. ', x, '\\n')\n",
        "\n",
        "        \"\"\" Cross entropy loss and perplexity on the training set \"\"\"\n",
        "        loss_train, num_words = train_loop_baseline(train_loader, optimizer, prediction_loss, baseline_model)\n",
        "\n",
        "        # Cross entropy\n",
        "        ce_train = np.asarray(loss_train).sum() / np.asarray(num_words).sum()\n",
        "        print('CE on the training set: ', ce_train, '\\n')\n",
        "\n",
        "        # Perplexity\n",
        "        perplexity_train = np.exp(ce_train)\n",
        "        print('Perplexity on the training set: ', perplexity_train, '\\n')\n",
        "\n",
        "        if x % 3 == 0:\n",
        "\n",
        "            sampled_epochs_baseline.append(x)\n",
        "            losses_train_baseline.append(ce_train)\n",
        "\n",
        "            \"\"\" Cross entropy loss and perplexity on the validation set \"\"\"\n",
        "            loss_valid, num_words = eval_loop_baseline(valid_loader, prediction_loss, baseline_model)\n",
        "\n",
        "            # Cross entropy\n",
        "            ce_valid = np.asarray(loss_valid).sum() / np.asarray(num_words).sum()\n",
        "            print('CE on the validation set: ', ce_valid, '\\n')\n",
        "\n",
        "            # Perplexity\n",
        "            perplexity_valid = np.exp(ce_valid)\n",
        "            print('Perplexity on the validation set: ', perplexity_valid, '\\n')\n",
        "\n",
        "            losses_valid_baseline.append(ce_valid)\n",
        "\n",
        "            if ce_valid < best_loss:\n",
        "                best_loss = ce_valid\n",
        "            else:\n",
        "                patience -= 1\n",
        "            if patience <= 0: \n",
        "                # Early stopping with patience\n",
        "                break \n",
        "\n",
        "\n",
        "    \"\"\" Cross entropy loss and perplexity on the test set \"\"\"\n",
        "    loss_test, num_words = eval_loop_baseline(test_loader, prediction_loss, baseline_model)\n",
        "\n",
        "    # Cross entropy\n",
        "    ce_test = np.asarray(loss_test).sum() / np.asarray(num_words).sum()\n",
        "    print('CE on the test set: ', ce_test, '\\n')\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity_test = np.exp(ce_test)\n",
        "    print('Perplexity on the test set: ', perplexity_test, '\\n')\n",
        "\n",
        "\n",
        "    return losses_train_baseline, losses_valid_baseline, sampled_epochs_baseline, baseline_model\n"
      ],
      "metadata": {
        "id": "u7Pq0B08kCJw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Training of the neural network\n",
        "\n",
        "In order to train the two baseline Neural Language Models, the previously defined **main** procedure is invoked, but some important elements and hyperparameters must be defined in advance:\n",
        "\n",
        "*  the **model architecture**, of which the hidden size, embedding size and vocabulary size must be defined. In this case, this values were set to standard ones, that seem to work well in general.\n",
        "\n",
        "*  the **optimizer**: **Adam** optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments, and in particular it computes a exponentially decaying moving average of squared gradients that is useful for the computation of **adaptive learning rates**, starting from a fixed initial value. This optimizer has shown to work well for this task, and the learning rate **lr** is initialized to the value that has shown to perform best in practice.\n",
        "\n",
        "*   the **prediction loss**, which is used for both training and evaluation, is the **Cross Entropy Loss**, and it also allows the computation of the **Perplexity** as $pp = exp(loss_{CrossEnt})$. In particular, it defined by requiring:\n",
        "  - **ignore_index = PAD_TOKEN**: the **pad** tokens present in the target vectors must be ignored for a correct computation of the loss and perplexity values, and this constraints the method to do so\n",
        "  - **reduction = 'sum'**: after adding up the cross entropy values corresponding to all the samples in the batch, the obtained value is returned, hence it is not subsequently divided for the total number of samples for the computation of the average. This allows to better handle the case in which different batches contain a different number of elements on which the Cross Entropy loss is computed (because of the presence of **pad** tokens): it is better to **sum up all these CE costs** and then divide, in the end, with respect to the **sum of the number of samples contained in all batches**.\n",
        "\n",
        "<br/>\n",
        "\n",
        "1.  **Vanilla RNN**"
      ],
      "metadata": {
        "id": "T6RGtsSKwi9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters \n",
        "n_epochs = 200        # Number of epochs\n",
        "patience = 3          # Patience parameter for the early stopping regularization\n",
        "hid_size = 200        # Size of the hidden LSTM representation \n",
        "emb_size = 300        # Size of the embedding vector\n",
        "lr = 0.001           # Learning rate for the optimization phase\n",
        "\n",
        "# Computation of the length of the vocabulary \n",
        "vocab_len = len(vocab.word2id)\n",
        "\n",
        "# Definition of the model, initialization of the weights\n",
        "rnn_model = Baseline_NLM(hid_size, emb_size, vocab_len, lstm = False, pad_index=PAD_TOKEN).to(device)\n",
        "rnn_model.apply(init_weights)\n",
        "\n",
        "# Definition of the optimizer\n",
        "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=lr)\n",
        "\n",
        "# Definition of the loss\n",
        "prediction_loss = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN, reduction = 'sum')\n",
        "\n",
        "# Training procedure\n",
        "loss_train_rnn, loss_valid_rnn, sampled_epochs_rnn, rnn_model = main_baseline(rnn_model,\n",
        "                                                                    prediction_loss, \n",
        "                                                                    optimizer, \n",
        "                                                                    train_loader, \n",
        "                                                                    valid_loader, \n",
        "                                                                    test_loader,\n",
        "                                                                    n_epochs = n_epochs,   \n",
        "                                                                    patience = patience)\n",
        "# Save the trained model \n",
        "torch.save(rnn_model.state_dict(), \"gdrive/MyDrive/NLU_project/models/baseline_model_rnn\")"
      ],
      "metadata": {
        "id": "dVBwpgOAuNfq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6dacbd7-3b8e-49ce-ab55-8026c98e6403"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/200 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch n.  0 \n",
            "\n",
            "CE on the training set:  6.0501240307359625 \n",
            "\n",
            "Perplexity on the training set:  424.16563635834444 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/200 [00:22<1:14:50, 22.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  5.491592544661883 \n",
            "\n",
            "Perplexity on the validation set:  242.64331964886566 \n",
            "\n",
            "Epoch n.  1 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 2/200 [00:44<1:13:59, 22.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  5.226421593342932 \n",
            "\n",
            "Perplexity on the training set:  186.12557742369725 \n",
            "\n",
            "Epoch n.  2 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 3/200 [01:06<1:12:37, 22.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.905267584040353 \n",
            "\n",
            "Perplexity on the training set:  134.99902875975346 \n",
            "\n",
            "Epoch n.  3 \n",
            "\n",
            "CE on the training set:  4.691743052109823 \n",
            "\n",
            "Perplexity on the training set:  109.04308202879348 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 4/200 [01:28<1:12:10, 22.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.985055430569861 \n",
            "\n",
            "Perplexity on the validation set:  146.21167939824946 \n",
            "\n",
            "Epoch n.  4 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▎         | 5/200 [01:50<1:11:20, 21.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.521399718524926 \n",
            "\n",
            "Perplexity on the training set:  91.9642319705838 \n",
            "\n",
            "Epoch n.  5 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 6/200 [02:11<1:10:28, 21.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.3773493144590025 \n",
            "\n",
            "Perplexity on the training set:  79.62668811301592 \n",
            "\n",
            "Epoch n.  6 \n",
            "\n",
            "CE on the training set:  4.247953508481243 \n",
            "\n",
            "Perplexity on the training set:  69.96208892001134 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 7/200 [02:33<1:10:19, 21.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.941136647855918 \n",
            "\n",
            "Perplexity on the validation set:  139.92920943807454 \n",
            "\n",
            "Epoch n.  7 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 8/200 [02:55<1:09:57, 21.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.132246281278573 \n",
            "\n",
            "Perplexity on the training set:  62.31774902455931 \n",
            "\n",
            "Epoch n.  8 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 9/200 [03:17<1:09:13, 21.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.024721051549855 \n",
            "\n",
            "Perplexity on the training set:  55.96469533904489 \n",
            "\n",
            "Epoch n.  9 \n",
            "\n",
            "CE on the training set:  3.923852880548698 \n",
            "\n",
            "Perplexity on the training set:  50.59500624175248 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 10/200 [03:39<1:09:10, 21.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  5.00902420262888 \n",
            "\n",
            "Perplexity on the validation set:  149.7585308465701 \n",
            "\n",
            "Epoch n.  10 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 11/200 [04:00<1:08:23, 21.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  3.8300409682262537 \n",
            "\n",
            "Perplexity on the training set:  46.06442537335878 \n",
            "\n",
            "Epoch n.  11 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 12/200 [04:22<1:07:41, 21.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  3.7405022536471586 \n",
            "\n",
            "Perplexity on the training set:  42.11913934477707 \n",
            "\n",
            "Epoch n.  12 \n",
            "\n",
            "CE on the training set:  3.658266379857799 \n",
            "\n",
            "Perplexity on the training set:  38.79403042853968 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▋         | 13/200 [04:43<1:07:34, 21.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  5.133381334823173 \n",
            "\n",
            "Perplexity on the validation set:  169.58958882409345 \n",
            "\n",
            "Epoch n.  13 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 14/200 [05:05<1:07:05, 21.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  3.578834238692266 \n",
            "\n",
            "Perplexity on the training set:  35.8317452277631 \n",
            "\n",
            "Epoch n.  14 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 15/200 [05:27<1:06:42, 21.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  3.5045649959664686 \n",
            "\n",
            "Perplexity on the training set:  33.26696943852867 \n",
            "\n",
            "Epoch n.  15 \n",
            "\n",
            "CE on the training set:  3.436429438843912 \n",
            "\n",
            "Perplexity on the training set:  31.075801790313413 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 15/200 [05:49<1:11:44, 23.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  5.2623396836486895 \n",
            "\n",
            "Perplexity on the validation set:  192.93236433957387 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the test set:  5.179098321318992 \n",
            "\n",
            "Perplexity on the test set:  177.5226703992398 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "In order to **assess the reliability** of the obtained result for the test loss and perplexity, at this point multiple runs (**5**) of training and test of the aforementioned model are executed, maintaining exactly the same setting described above for each run.  \n",
        "\n",
        "At the end of each run the test loss and perplexity values are stored, and in the end the results' **mean** and **standard deviation** is computed and printed. "
      ],
      "metadata": {
        "id": "Nf5PdOqyDkeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the number of runs\n",
        "runs = 5 \n",
        "\n",
        "test_loss_runs, test_perplexity_runs = [], []\n",
        "\n",
        "for x in tqdm(range(0, runs)):\n",
        "\n",
        "    # Hyperparameters \n",
        "    n_epochs = 200        # Number of epochs\n",
        "    patience = 3          # Patience parameter for the early stopping regularization\n",
        "    hid_size = 200        # Size of the hidden LSTM representation \n",
        "    emb_size = 300        # Size of the embedding vector\n",
        "    lr = 0.001           # Learning rate for the optimization phase\n",
        "\n",
        "    # Computation of the length of the vocabulary \n",
        "    vocab_len = len(vocab.word2id)\n",
        "\n",
        "    # Definition of the model, initialization of the weights\n",
        "    rnn_model = Baseline_NLM(hid_size, emb_size, vocab_len, lstm = False, pad_index=PAD_TOKEN).to(device)\n",
        "    rnn_model.apply(init_weights)\n",
        "\n",
        "    # Definition of the optimizer\n",
        "    optimizer = torch.optim.Adam(rnn_model.parameters(), lr=lr)\n",
        "\n",
        "    # Definition of the loss\n",
        "    prediction_loss = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN, reduction = 'sum')\n",
        "    \n",
        "\n",
        "    # Initial best loss value, used for the early stopping technique\n",
        "    best_loss = 100.00\n",
        "\n",
        "\n",
        "    for x in range(1, n_epochs):\n",
        "\n",
        "        # For each epoch:\n",
        "        \"\"\" Training step \"\"\"\n",
        "        loss_train, num_words = train_loop_baseline(train_loader, optimizer, prediction_loss, rnn_model)\n",
        "\n",
        "        if x % 3 == 0:\n",
        "\n",
        "            \"\"\" Validation step \"\"\"\n",
        "            loss_valid, num_words = eval_loop_baseline(valid_loader, prediction_loss, rnn_model)\n",
        "\n",
        "            # Cross entropy\n",
        "            ce_valid = np.asarray(loss_valid).sum() / np.asarray(num_words).sum()\n",
        "\n",
        "            if ce_valid < best_loss:\n",
        "                best_loss = ce_valid\n",
        "            else:\n",
        "                patience -= 1\n",
        "            if patience <= 0: \n",
        "                # Early stopping with patience\n",
        "                break \n",
        "\n",
        "\n",
        "    \"\"\" Cross entropy loss and perplexity on the test set \"\"\"\n",
        "    loss_test, num_words = eval_loop_baseline(test_loader, prediction_loss, rnn_model)\n",
        "\n",
        "    # Cross entropy\n",
        "    ce_test = np.asarray(loss_test).sum() / np.asarray(num_words).sum()\n",
        "    test_loss_runs.append(ce_test)\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity_test = np.exp(ce_test)\n",
        "    test_perplexity_runs.append(perplexity_test)\n",
        "    print(perplexity_test)\n",
        "\n",
        "test_loss_runs = np.asarray(test_loss_runs)\n",
        "test_perplexity_runs = np.asarray(test_perplexity_runs)\n",
        "\n",
        "# Computation of mean and standard deviation values among the results of the 5 runs\n",
        "print('Test loss', round(test_loss_runs.mean(),3), '+-', round(test_loss_runs.std(),3))\n",
        "print('Test perplexity', round(test_perplexity_runs.mean(), 3), '+-', round(test_perplexity_runs.std(), 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgQXEYrn__D-",
        "outputId": "b8bac6e1-76ae-48cf-b21d-adc78f50d961"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [05:23<21:32, 323.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "171.22848479671384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [10:47<16:10, 323.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169.9562736615147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [16:15<10:51, 325.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "170.84788931706342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [21:43<05:26, 326.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "167.31326839890806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [27:06<00:00, 325.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "168.5247761496744\n",
            "Test loss 5.133 +- 0.009\n",
            "Test perplexity 169.574 +- 1.464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "2.  **LSTM**"
      ],
      "metadata": {
        "id": "Fms94r5EvZnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters \n",
        "n_epochs = 200        # Number of epochs\n",
        "patience = 3          # Patience parameter for the early stopping regularization\n",
        "hid_size = 200        # Size of the hidden LSTM representation \n",
        "emb_size = 300        # Size of the embedding vector\n",
        "lr = 0.001           # Learning rate for the optimization phase\n",
        "\n",
        "# Computation of the length of the vocabulary \n",
        "vocab_len = len(vocab.word2id)\n",
        "\n",
        "# Definition of the model, initialization of the weights\n",
        "lstm_model = Baseline_NLM(hid_size, emb_size, vocab_len, lstm = True, pad_index=PAD_TOKEN).to(device)\n",
        "lstm_model.apply(init_weights)\n",
        "\n",
        "# Definition of the optimizer\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
        "\n",
        "# Definition of the loss\n",
        "prediction_loss = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN, reduction = 'sum')\n",
        "\n",
        "# Training procedure\n",
        "loss_train_lstm, loss_valid_lstm, sampled_epochs_lstm, lstm_model = main_baseline(lstm_model,\n",
        "                                                                    prediction_loss, \n",
        "                                                                    optimizer, \n",
        "                                                                    train_loader, \n",
        "                                                                    valid_loader, \n",
        "                                                                    test_loader,\n",
        "                                                                    n_epochs = n_epochs,   \n",
        "                                                                    patience = patience)\n",
        "# Save the trained model \n",
        "torch.save(lstm_model.state_dict(), \"gdrive/MyDrive/NLU_project/models/baseline_model_lstm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tzT8EQ9ofAZ",
        "outputId": "2f71a19f-bb7b-4951-883e-189e54d2a9a3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/200 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch n.  0 \n",
            "\n",
            "CE on the training set:  6.192854406204719 \n",
            "\n",
            "Perplexity on the training set:  489.24060645521377 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/200 [00:23<1:16:50, 23.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  5.668114145297627 \n",
            "\n",
            "Perplexity on the validation set:  289.4880868235107 \n",
            "\n",
            "Epoch n.  1 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 2/200 [00:46<1:15:48, 22.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  5.438467635554539 \n",
            "\n",
            "Perplexity on the training set:  230.08933246917485 \n",
            "\n",
            "Epoch n.  2 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 3/200 [01:08<1:14:55, 22.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  5.135568686846344 \n",
            "\n",
            "Perplexity on the training set:  169.96094695172874 \n",
            "\n",
            "Epoch n.  3 \n",
            "\n",
            "CE on the training set:  4.934692715386057 \n",
            "\n",
            "Perplexity on the training set:  139.03041406040285 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 4/200 [01:31<1:14:50, 22.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  5.079437799530962 \n",
            "\n",
            "Perplexity on the validation set:  160.68369408207136 \n",
            "\n",
            "Epoch n.  4 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▎         | 5/200 [01:54<1:13:54, 22.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.780990333086352 \n",
            "\n",
            "Perplexity on the training set:  119.22236144904574 \n",
            "\n",
            "Epoch n.  5 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 6/200 [02:16<1:13:15, 22.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.6518387971594475 \n",
            "\n",
            "Perplexity on the training set:  104.77747307000567 \n",
            "\n",
            "Epoch n.  6 \n",
            "\n",
            "CE on the training set:  4.537855988880739 \n",
            "\n",
            "Perplexity on the training set:  93.49014118660241 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 7/200 [02:39<1:13:19, 22.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.9380578399974695 \n",
            "\n",
            "Perplexity on the validation set:  139.49905680686746 \n",
            "\n",
            "Epoch n.  7 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 8/200 [03:02<1:12:38, 22.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.434731833353494 \n",
            "\n",
            "Perplexity on the training set:  84.32950749458455 \n",
            "\n",
            "Epoch n.  8 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 9/200 [03:25<1:12:28, 22.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.339932480569914 \n",
            "\n",
            "Perplexity on the training set:  76.7023602638022 \n",
            "\n",
            "Epoch n.  9 \n",
            "\n",
            "CE on the training set:  4.251582460492704 \n",
            "\n",
            "Perplexity on the training set:  70.21643921669967 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 10/200 [03:48<1:12:24, 22.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.8973416419069205 \n",
            "\n",
            "Perplexity on the validation set:  133.9332634461609 \n",
            "\n",
            "Epoch n.  10 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 11/200 [04:10<1:11:41, 22.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.168115377389882 \n",
            "\n",
            "Perplexity on the training set:  64.59360275922114 \n",
            "\n",
            "Epoch n.  11 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 12/200 [04:33<1:11:09, 22.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.089755700807009 \n",
            "\n",
            "Perplexity on the training set:  59.72529907936833 \n",
            "\n",
            "Epoch n.  12 \n",
            "\n",
            "CE on the training set:  4.01415733157542 \n",
            "\n",
            "Perplexity on the training set:  55.37661161371189 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▋         | 13/200 [04:56<1:11:14, 22.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.924197755779185 \n",
            "\n",
            "Perplexity on the validation set:  137.5789254605519 \n",
            "\n",
            "Epoch n.  13 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 14/200 [05:19<1:10:41, 22.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  3.941852862451635 \n",
            "\n",
            "Perplexity on the training set:  51.513961213311475 \n",
            "\n",
            "Epoch n.  14 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 15/200 [05:42<1:10:19, 22.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  3.872403522457805 \n",
            "\n",
            "Perplexity on the training set:  48.0577552731277 \n",
            "\n",
            "Epoch n.  15 \n",
            "\n",
            "CE on the training set:  3.8047845835577854 \n",
            "\n",
            "Perplexity on the training set:  44.915573517754765 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 16/200 [06:05<1:10:12, 22.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.987621095692304 \n",
            "\n",
            "Perplexity on the validation set:  146.58729124564053 \n",
            "\n",
            "Epoch n.  16 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 17/200 [06:27<1:09:31, 22.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  3.739506977488948 \n",
            "\n",
            "Perplexity on the training set:  42.0772400237392 \n",
            "\n",
            "Epoch n.  17 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 18/200 [06:50<1:08:54, 22.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  3.6763910137132934 \n",
            "\n",
            "Perplexity on the training set:  39.50356866254286 \n",
            "\n",
            "Epoch n.  18 \n",
            "\n",
            "CE on the training set:  3.6146786730576745 \n",
            "\n",
            "Perplexity on the training set:  37.13941011500843 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 18/200 [07:13<1:13:00, 24.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  5.0700697779503034 \n",
            "\n",
            "Perplexity on the validation set:  159.18543458911597 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the test set:  4.96249238862453 \n",
            "\n",
            "Perplexity on the test set:  142.94963831640266 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "In order to **assess the reliability** of the obtained result for the test loss and perplexity, at this point multiple runs (**5**) of training and test of the aforementioned model are executed, maintaining exactly the same setting described above for each run.  \n",
        "\n",
        "At the end of each run the test loss and perplexity values are stored, and in the end the results' **mean** and **standard deviation** is computed and printed. "
      ],
      "metadata": {
        "id": "dgarbZN-E2WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the number of runs\n",
        "runs = 5 \n",
        "\n",
        "test_loss_runs, test_perplexity_runs = [], []\n",
        "\n",
        "for x in tqdm(range(0, runs)):\n",
        "\n",
        "    # Hyperparameters \n",
        "    n_epochs = 200        # Number of epochs\n",
        "    patience = 3          # Patience parameter for the early stopping regularization\n",
        "    hid_size = 200        # Size of the hidden LSTM representation \n",
        "    emb_size = 300        # Size of the embedding vector\n",
        "    lr = 0.001           # Learning rate for the optimization phase\n",
        "\n",
        "    # Computation of the length of the vocabulary \n",
        "    vocab_len = len(vocab.word2id)\n",
        "\n",
        "    # Definition of the model, initialization of the weights\n",
        "    lstm_model = Baseline_NLM(hid_size, emb_size, vocab_len, lstm = True, pad_index=PAD_TOKEN).to(device)\n",
        "    lstm_model.apply(init_weights)\n",
        "\n",
        "    # Definition of the optimizer\n",
        "    optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
        "\n",
        "    # Definition of the loss\n",
        "    prediction_loss = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN, reduction = 'sum')\n",
        "    \n",
        "\n",
        "    # Initial best loss value, used for the early stopping technique\n",
        "    best_loss = 100.00\n",
        "\n",
        "\n",
        "    for x in range(1, n_epochs):\n",
        "\n",
        "        # For each epoch:\n",
        "        \"\"\" Training step \"\"\"\n",
        "        loss_train, num_words = train_loop_baseline(train_loader, optimizer, prediction_loss, lstm_model)\n",
        "\n",
        "        if x % 3 == 0:\n",
        "\n",
        "            \"\"\" Validation step \"\"\"\n",
        "            loss_valid, num_words = eval_loop_baseline(valid_loader, prediction_loss, lstm_model)\n",
        "\n",
        "            # Cross entropy\n",
        "            ce_valid = np.asarray(loss_valid).sum() / np.asarray(num_words).sum()\n",
        "\n",
        "            if ce_valid < best_loss:\n",
        "                best_loss = ce_valid\n",
        "            else:\n",
        "                patience -= 1\n",
        "            if patience <= 0: \n",
        "                # Early stopping with patience\n",
        "                break \n",
        "\n",
        "\n",
        "    \"\"\" Cross entropy loss and perplexity on the test set \"\"\"\n",
        "    loss_test, num_words = eval_loop_baseline(test_loader, prediction_loss, lstm_model)\n",
        "\n",
        "    # Cross entropy\n",
        "    ce_test = np.asarray(loss_test).sum() / np.asarray(num_words).sum()\n",
        "    test_loss_runs.append(ce_test)\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity_test = np.exp(ce_test)\n",
        "    test_perplexity_runs.append(perplexity_test)\n",
        "    print(perplexity_test)\n",
        "\n",
        "test_loss_runs = np.asarray(test_loss_runs)\n",
        "test_perplexity_runs = np.asarray(test_perplexity_runs)\n",
        "\n",
        "# Computation of mean and standard deviation values among the results of the 5 runs\n",
        "print('Test loss', round(test_loss_runs.mean(),3), '+-', round(test_loss_runs.std(),3))\n",
        "print('Test perplexity', round(test_perplexity_runs.mean(), 3), '+-', round(test_perplexity_runs.std(), 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b033d2c-b3c3-4bb4-98a7-677bd1b44294",
        "id": "sF_XdMtWE2WE"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [06:48<27:15, 408.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "138.6628835090754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [13:36<20:25, 408.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "137.09937832826287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [20:24<13:36, 408.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "138.47082958356805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [27:13<06:48, 408.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "139.76938786909454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [34:00<00:00, 408.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "138.2187649278532\n",
            "Test loss 4.93 +- 0.006\n",
            "Test perplexity 138.444 +- 0.856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Plot of the training and validation losses for the baseline models\n",
        "\n",
        "The plots clearly show that, although the LSTM architecture reaches a final value for the test perplexity which is better than the RNN, both the baseline models highly suffer from **overfitting** and **low generalization** capabilities.\n",
        "\n",
        "This implies that strong **regularization** techniques must be introduced, as well as an improved **optimization** procedure. \n"
      ],
      "metadata": {
        "id": "mcjA83yX1eR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vanilla RNN\n",
        "\n",
        "plt.figure(num = 3, figsize=(8, 5)).patch.set_facecolor('white')\n",
        "plt.title('Baseline Vanilla RNN: Train and Valid Losses')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.plot(sampled_epochs_rnn, loss_train_rnn, label='Train loss Vanilla RNN')\n",
        "plt.plot(sampled_epochs_rnn, loss_valid_rnn, label='Valid loss Vanilla RNN')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# LSTM\n",
        "\n",
        "plt.figure(num = 3, figsize=(8, 5)).patch.set_facecolor('white')\n",
        "plt.title('Baseline LSTM: Train and Valid Losses')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.plot(sampled_epochs_lstm, loss_train_lstm, label='Train loss LSTM')\n",
        "plt.plot(sampled_epochs_lstm, loss_valid_lstm, label='Valid loss LSTM')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "voCXLa1y1Udx",
        "outputId": "a63f18fd-a8af-49c9-af10-60fd6ee827cc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "## <center>Improved LSTM Models</center>"
      ],
      "metadata": {
        "id": "X8fZ51FlosjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Definition of the improved LSTM model\n",
        "\n",
        "In order to improve the performance and results of the baseline models, I decided to introduce some **regularization** techniques, as well as improve the optimization algorithm, to the training phase of the **LSTM** model.\n",
        "\n",
        "In particular, the additions I made to the **definition of the architecture** are the following:\n",
        "\n",
        "* **Embedding dropout** regularization technique to the embedding layer. In general, dropout avoids overfitting by adding some noise in the training phase (the value of **0.1** was suggested by the *Regularizing and Optimizing LSTM Language Models* paper).\n",
        "\n",
        "* **Locked dropout** regularization technique to the inputs and outputs of the LSTM layer: it samples a\n",
        "binary dropout mask only once upon the first call and then\n",
        "it repeatedly uses that locked dropout mask for all repeated\n",
        "connections within the forward and backward pass. The values of **0.4** were suggested by the *Regularizing and Optimizing LSTM Language Models* paper.\n",
        "\n",
        "* **Weight tying** of the weights in **embedding** matrix and **decoder** output matrix: this technique allows to substantially reduce the number of parameters of the model, reducing the problem of overfitting and improving the performance in the training procedure.\n",
        "\n",
        "* **Attention module**: although attention modules are mostly used in the framework of machine translation, because of their ability of providing a good measure for the alignment between the words in the input sentence and translated sentence, I tried implementing a sort of attention layer that could suit and be useful for the considered Language Modeling task. I took inspiration, to some extent, from the general ideas underlying the structure of the **self-attention** module in the Transformer proposed in the *Attention Is All You Need* paper.\n",
        "\n",
        "* **Residual connections**: as shown with their introduction in ResNet, skip connections implemented through residual blocks improve the results obtained from the training procedure because they help the model in better exploiting its parameters in order to approximate certain complex functions, and they allow to skip some layers when they are not needed. Also, these residual connections encourage **feature reuse**. \n",
        "In particular, the whole aforementioned **attention module** is introduced in the architecture as a residual block, hence it can be ignored by the network if it's considered not useful.\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=13QJYClkMM3wd824BtBmNR9UBTw_HNKjZ' width='500'>\n",
        "</center>"
      ],
      "metadata": {
        "id": "1IsFkETT8lxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "class LockedDropout(nn.Module):\n",
        "# source: https://github.com/salesforce/awd-lstm-lm/blob/master/locked_dropout.py\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, dropout=0.5):\n",
        "        if not self.training or not dropout:\n",
        "            return x\n",
        "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
        "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, hid_size):\n",
        "\n",
        "        super(AttentionLayer, self).__init__()  \n",
        "\n",
        "        self.energy = nn.Bilinear(hid_size, hid_size, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.value = nn.Linear(hid_size, hid_size)\n",
        "\n",
        "    def forward(self, hidden_timesteps):\n",
        "        # input: (seq_len, batch_size, hid_size)\n",
        "        n_timesteps = hidden_timesteps.shape[0]\n",
        "        #batch_size = hidden_timesteps.shape[1]\n",
        "        #hid_size = hidden_timesteps.shape[2]\n",
        "\n",
        "        context_vectors = torch.zeros_like(hidden_timesteps)\n",
        "        context_vectors[0] = hidden_timesteps[0]\n",
        "\n",
        "        for t in range(1, n_timesteps):\n",
        "            # For each timestep's hidden state:\n",
        "\n",
        "            s = max(t-6, 0)\n",
        "\n",
        "            hidden_previous_t = hidden_timesteps[s:t+1]\n",
        "            # hidden_previous_t: (t-s+1, batch_size, hid_size)\n",
        "\n",
        "            hidden_t = hidden_timesteps[t].repeat(t-s+1, 1, 1)\n",
        "            # hidden_t: (t-s+1, batch_size, hid_size)\n",
        "\n",
        "            alignment_scores = self.energy(hidden_t, hidden_previous_t)\n",
        "            # alignment_scores: (t-s+1, batch_size, 1)\n",
        "\n",
        "            attention = self.softmax(alignment_scores)\n",
        "            # attention: (t-s+1, batch_size, 1)\n",
        "\n",
        "            # Residual block with value computation\n",
        "            values = self.relu(self.value(hidden_previous_t))\n",
        "            hidden_previous_t = hidden_previous_t + values\n",
        "\n",
        "            # attention: (t-s+1, batch_size, 1) ----------------------> tnk\n",
        "            # hidden_previous_t: (t-s+1, batch_size, hid_size) -------> tnh\n",
        "            # we want context_vectors[t]: (1, batch_size, hid_size) --> knh\n",
        "            context_vectors[t] = torch.einsum(\"tnk,tnh->knh\", attention, hidden_previous_t)\n",
        "\n",
        "        return context_vectors\n",
        "\n",
        "\n",
        "class Improved_LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, hid_size, emb_size, vocab_len, attention = True, n_layer = 1, pad_index = 0, tie_weights=True):\n",
        "\n",
        "        super(Improved_LSTM, self).__init__()  \n",
        "\n",
        "        self.hid_size = hid_size\n",
        "        self.emb_size = emb_size\n",
        "        self.vocab_len = vocab_len\n",
        "        self.n_layer = n_layer\n",
        "        self.pad_index = pad_index\n",
        "\n",
        "        # Dropout regularization \n",
        "        # avoids overfitting adding some noise in training phase\n",
        "        self.lockdrop = LockedDropout()\n",
        "        self.dropout_emb = nn.Dropout(0.1) \n",
        "        self.drop_inp = 0.4\n",
        "        self.drop_out = 0.4\n",
        "\n",
        "        # Encoder layer: word embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
        "        \n",
        "        # We set bidirectionality to false, so the LSTM is one-directional \n",
        "        self.lstm = nn.LSTM(emb_size, hid_size, num_layers = n_layer, bidirectional=False)    \n",
        "\n",
        "        # Attention layer\n",
        "        self.attention = attention\n",
        "        if self.attention:\n",
        "            self.attention_layer = AttentionLayer(hid_size)\n",
        "\n",
        "        # Decoder layer: linear output layer\n",
        "        self.decoder = nn.Linear(hid_size, vocab_len)\n",
        "\n",
        "        # Weight tying regularization technique\n",
        "        if tie_weights:\n",
        "            self.decoder.weight = self.embedding.weight\n",
        "        \n",
        "    def forward(self, sentences, seq_lengths, hidden = None, cell = None):\n",
        "\n",
        "        # Dropout applied to the first layer\n",
        "        # sentences: (batch_size, seq_len)\n",
        "        sents_emb = self.dropout_emb(self.embedding(sentences)) \n",
        "        # sents_emb: (batch_size, seq_len, emb_size)\n",
        "\n",
        "        # Swap the matrix in order to have dimensions setted in another way\n",
        "        # We need seq len first -> permute \n",
        "        sents_emb = sents_emb.permute(1,0,2) \n",
        "        # sents_emb: (seq_len, batch_size, emb_size)\n",
        "        sents_emb = self.lockdrop(sents_emb, self.drop_inp)\n",
        "\n",
        "        # Compression\n",
        "        # pack_padded_sequence avoids computation over pad tokens reducing the computational cost\n",
        "        packed_input = pack_padded_sequence(sents_emb, seq_lengths.cpu().numpy())\n",
        "\n",
        "        # Process the batch through the LSTM layer\n",
        "        if hidden == None:\n",
        "          packed_output, (last_hidden, last_cell) = self.lstm(packed_input) \n",
        "        else:\n",
        "          hidden = hidden.detach()\n",
        "          cell = cell.detach()\n",
        "          packed_output, (last_hidden, last_cell) = self.lstm(packed_input, (hidden, cell)) \n",
        "\n",
        "\n",
        "        # Unpack the sequence: Decompression\n",
        "        padded_output, _ = pad_packed_sequence(packed_output)\n",
        "\n",
        "        lstm_hs = padded_output\n",
        "        padded_output = self.lockdrop(padded_output, self.drop_out)\n",
        "        dropped_lstm_hs = padded_output\n",
        "\n",
        "        # Residual block with attention layer \n",
        "        if self.attention:\n",
        "            identity = padded_output\n",
        "            attention_output = self.attention_layer(padded_output)\n",
        "            attention_output = attention_output + identity\n",
        "        else:\n",
        "            attention_output = padded_output\n",
        "          \n",
        "        # Get the output\n",
        "        out = self.decoder(attention_output)\n",
        "\n",
        "        # Swap the output in order to have dimensions setted correctly for the loss\n",
        "        out = out.permute(1,0,2).contiguous()\n",
        "        out = out.view(-1, out.shape[-1])\n",
        "        # out: (batch_size * seq_len, vocab_len)\n",
        "\n",
        "        return out, last_hidden, last_cell, lstm_hs, dropped_lstm_hs"
      ],
      "metadata": {
        "id": "ZDq8IxmtflpL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Training loop\n",
        "\n",
        "As done for the architectural design, also in the training pipeline some additional **regularization** and **optimization** techniques are introduced to improve the training and achieve a better performing model.\n",
        "\n",
        "* A very simple but effective adjustment, which is fundamental for reducing the impact of exploding gradients during training, is represented by **gradient clipping**. The problem of exploding gradients is particularly common with recurrent neural networks, such as LSTMs given the accumulation of gradients unrolled over all the input timesteps. With gradient clipping, pre-determined gradient threshold be introduced, and  then gradients norms that exceed this threshold are scaled down, making the training phase more stable.\n",
        "\n",
        "* As well as introducing the **weight decay** regularization technique to reduce the problem of overfitting, $L_2$ decay is used also on the individual\n",
        "unit activations and on the difference in outputs\n",
        "of the LSTM at different time steps. These last two strategies are labeled\n",
        "as **Activation Regularization** (AR) and **Temporal Activation\n",
        "Regularization** (TAR) respectively. Their influence is regulated by the **alpha** and **beta** hyperparameters, and their application was suggested by the *Regularizing and Optimizing LSTM Language Models* paper, which also provides their definition.\n",
        "\n",
        "In this case, differently from the baseline setting, a sort of **Truncated Backpropagation Through Time** is implemented in order to obtain better gradients during training, reduce the problem of vanishing and exploding gradients, speed up training and converge faster. Indeed, the same amount of improvements in the training perplexity happens **at least 10 epochs before** with respect to the case of simple BPTT.\n",
        "\n",
        "Only one truncation is applied to the batch, and it is performed **after the 25th position** of the sentences. The first sub-batch obtained from the original one, in which the maximum sentence length is 25, is passed as input to the RNN model and backpropagation and weight update are performed. After that, before considering the second part of the truncated batch, the final values of the hidden state and cell state of the LSTM are **detached from the computational graph** (while their value is maintained unaltered), in order to avoid considering the first subset of timesteps for the computation of future gradients. Besides being detached, they are also **resized** in order to maintain just the ones that are necessary for the processing of the second sub-batch (their batch size value will have to correspond to the second mini-batch size).\n",
        "\n",
        "The second sub-batch, composed by the remaining parts of the initial sentences, and that could have a **lower batch size** with respect to the initial one, is then passed as input to the RNN. The corresponding loss is then acquired and the backpropagation and weight update steps are performed, limiting the computation of errors and gradients to the timesteps of the current sub-batch.\n",
        "\n",
        "This clearly implies that if the maximum length of the sentences in the batch is less than 25, the truncation is not performed and a simple BackPropagation Through Time is applied to the batch. In this case, indeed, there is no particular need for TBPTT. \n",
        "The specific value of **25** was chosen by considering and studying the frequency distribution of the sentences' lengths for all sentences in the training set. Since the most frequent sentence length is around 20, and **most sentences have length values lower than 25**, this choice for the truncation threshold allows to perform just one backpropagation step for most sentences (the ones with length $<$ 25), while splitting and performing two BPTT steps just for the remaining sequences, that are much fewer. This condition grants a more stable and fast training with respect to setting the truncation threshold to a randomly chosen value.\n",
        "\n",
        "When a new batch, taken from the dataloader, is processed, the LSTM's hidden state and cell state are detached from the computational graph, while their values are **maintained unaltered**. This didn't happen in the baseline case, in which they were re-initialized to 0 with every new batch."
      ],
      "metadata": {
        "id": "XTRQ2vlaTSz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(data_loader, optimizer, cost_function, model, alpha = 2, beta = 1, clip = 0.25, hidden = None, cell = None, device = 'cuda:0'):\n",
        "\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Cumulative loss values obtained for each batch\n",
        "    loss_sum_array = []\n",
        "\n",
        "    # Total number of words in the batch\n",
        "    num_words_array = []\n",
        "\n",
        "    # Iterate over the training set\n",
        "    for batch_idx, (inputs, targets, lengths) in enumerate(data_loader):\n",
        "\n",
        "       if lengths[0].item() > 25:\n",
        "\n",
        "          inputs_1 = inputs[:, :25]\n",
        "          targets_1 = targets[:, :25]\n",
        "          lengths_1 = torch.LongTensor([min(25, k) for k in lengths.tolist()])\n",
        "          lengths_2 = torch.LongTensor([lengths[i] - lengths_1[i] for i in range(len(lengths)) if lengths[i] - lengths_1[i] != 0])\n",
        "          inputs_2 = inputs[:len(lengths_2), 25:]\n",
        "          targets_2 = targets[:len(lengths_2), 25:]\n",
        "\n",
        "          \"\"\" First half of minibatch \"\"\"\n",
        "\n",
        "          # Load data into GPU\n",
        "          inputs_1 = inputs_1.to(device)\n",
        "          targets_1 = targets_1.to(device)\n",
        "          lengths_1 = lengths_1.to(device)\n",
        "            \n",
        "          # Gradients reset\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          if hidden != None:\n",
        "            hidden = hidden.detach()[:, :len(lengths_1)].contiguous()\n",
        "            cell = cell.detach()[:, :len(lengths_1)].contiguous()\n",
        "          outputs_1, hidden, cell, rnn_hs, dropped_rnn_hs = model(inputs_1, lengths_1, hidden, cell)\n",
        "\n",
        "          # Loss computation\n",
        "          loss_1 = cost_function(outputs_1, targets_1.view(-1))\n",
        "\n",
        "          # Update of the loss array and number of words array\n",
        "          loss_sum_array.append(loss_1.item())\n",
        "          num_words_array.append((torch.sum(lengths_1)).item())\n",
        "\n",
        "          # Activation Regularization\n",
        "          loss_1 = loss_1 + (alpha * dropped_rnn_hs.pow(2).mean())\n",
        "\n",
        "          # Temporal Activation Regularization (slowness)\n",
        "          loss_1 = loss_1 + (beta * (rnn_hs[1:] - rnn_hs[:-1]).pow(2).mean())\n",
        "\n",
        "          # Backward pass\n",
        "          loss_1.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), clip) \n",
        "          \n",
        "          # Parameters update\n",
        "          optimizer.step()\n",
        "\n",
        "          \"\"\" Second half of minibatch \"\"\"\n",
        "\n",
        "          # Load data into GPU\n",
        "          inputs_2 = inputs_2.to(device)\n",
        "          targets_2 = targets_2.to(device)\n",
        "          lengths_2 = lengths_2.to(device)\n",
        "\n",
        "            \n",
        "          # Gradients reset\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass: the hidden state and cell are resized in order to \n",
        "          # fit the second mini-batch size\n",
        "          hidden = hidden.detach()\n",
        "          cell = cell.detach()\n",
        "          hidden_2 = hidden[:, :len(lengths_2)].contiguous() \n",
        "          cell_2 = cell[:, :len(lengths_2)].contiguous()\n",
        "          outputs_2, hidden_2, cell_2, rnn_hs, dropped_rnn_hs = model(inputs_2, lengths_2, hidden_2, cell_2)\n",
        "\n",
        "          # Loss computation\n",
        "          loss_2 = cost_function(outputs_2, targets_2.view(-1))\n",
        "\n",
        "          # Update of the loss array and number of words array\n",
        "          loss_sum_array.append(loss_2.item())\n",
        "          num_words_array.append((torch.sum(lengths_2)).item())\n",
        "\n",
        "          # Activation Regularization\n",
        "          loss_2 = loss_2 + (alpha * dropped_rnn_hs.pow(2).mean())\n",
        "\n",
        "          # Temporal Activation Regularization (slowness)\n",
        "          loss_2 = loss_2 + (beta * (rnn_hs[1:] - rnn_hs[:-1]).pow(2).mean())\n",
        "\n",
        "          # Backward pass\n",
        "          loss_2.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), clip) \n",
        "          \n",
        "          # Parameters update\n",
        "          optimizer.step()\n",
        "\n",
        "          hidden[:, :len(lengths_2)] = hidden_2\n",
        "          cell[:, :len(lengths_2)] = cell_2\n",
        "\n",
        "       else:\n",
        "\n",
        "          \"\"\" Entire mini-batch is processed \"\"\"\n",
        "\n",
        "          # Load data into GPU\n",
        "          inputs = inputs.to(device)\n",
        "          targets = targets.to(device)\n",
        "          lengths = lengths.to(device)\n",
        "\n",
        "          # Gradients reset\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          if hidden != None:\n",
        "            hidden = hidden.detach()[:, :len(lengths)].contiguous()\n",
        "            cell = cell.detach()[:, :len(lengths)].contiguous()\n",
        "          outputs, hidden, cell, rnn_hs, dropped_rnn_hs = model(inputs, lengths, hidden, cell)\n",
        "\n",
        "          # Loss computation\n",
        "          loss = cost_function(outputs, targets.view(-1))\n",
        "\n",
        "          # Update of the loss array and number of words array\n",
        "          loss_sum_array.append(loss.item())\n",
        "          num_words_array.append((torch.sum(lengths)).item())\n",
        "\n",
        "          # Activation Regularization\n",
        "          loss = loss + (alpha * dropped_rnn_hs.pow(2).mean())\n",
        "\n",
        "          # Temporal Activation Regularization (slowness)\n",
        "          loss = loss + (beta * (rnn_hs[1:] - rnn_hs[:-1]).pow(2).mean())\n",
        "\n",
        "          # Backward pass\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), clip) \n",
        "          \n",
        "          # Parameters update\n",
        "          optimizer.step()\n",
        "      \n",
        "    return loss_sum_array, num_words_array"
      ],
      "metadata": {
        "id": "WkS1rQCrTSz_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Evaluation loop\n",
        "\n",
        "The evaluation loop remains the same as the one adopted in the case of the baseline models. Indeed, the truncation of the batches into two sub-batches is useful only to improve the learning procedure when applying Backpropagation Through Time.\n",
        "\n",
        "The only difference from the baseline case is that the values of the hidden states and cell states of the LSTM layer are not re-initialized to 0 each time a new batch is processed; indeed, they are detached from the computational graph but their value is maintained unaltered. "
      ],
      "metadata": {
        "id": "eSpo33KvatRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_loop(data_loader, cost_function, model, hidden = None, cell = None, device = 'cuda:0'):\n",
        "\n",
        "  # Set the network to evaluation mode\n",
        "  model.eval() \n",
        "\n",
        "  # Cumulative loss values obtained for each batch\n",
        "  loss_sum_array = []\n",
        "\n",
        "  # Total number of words in the batch\n",
        "  num_words_array = []\n",
        "\n",
        "  # Disable gradient computation \n",
        "  with torch.no_grad():\n",
        "\n",
        "    # Iterate over the test set\n",
        "    for batch_idx, (inputs, targets, lengths) in enumerate(data_loader):\n",
        "      \n",
        "        # Load data into GPU\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        if hidden != None:\n",
        "            hidden = hidden[:, :len(lengths)].contiguous()\n",
        "            cell = cell[:, :len(lengths)].contiguous()\n",
        "        outputs, hidden, cell, _, _ = model(inputs, lengths, hidden, cell)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = cost_function(outputs, targets.view(-1))\n",
        "\n",
        "        # Update of the loss array\n",
        "        loss_sum_array.append(loss.item())\n",
        "        num_words_array.append((torch.sum(lengths)).item())\n",
        "      \n",
        "  return loss_sum_array, num_words_array"
      ],
      "metadata": {
        "id": "csl_pXudG8KH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Definition of the training procedure\n"
      ],
      "metadata": {
        "id": "qlu6rumE-ld-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(model, \n",
        "         prediction_loss,\n",
        "         optimizer, \n",
        "         train_loader, \n",
        "         valid_loader, \n",
        "         test_loader,\n",
        "         n_epochs = 200,   \n",
        "         patience = 3):\n",
        "\n",
        "    # Vectors filled during training that allow to plot the loss variations across epochs\n",
        "    losses_train = []\n",
        "    losses_valid = []\n",
        "    sampled_epochs = []\n",
        "\n",
        "    # Initial best loss value, used for the early stopping technique\n",
        "    best_loss = 100.00\n",
        "\n",
        "\n",
        "    for x in tqdm(range(1, n_epochs)):\n",
        "\n",
        "        # For each epoch:\n",
        "        print('Epoch n. ', x, '\\n')\n",
        "\n",
        "        \"\"\" Cross entropy loss and perplexity on the training set \"\"\"\n",
        "        loss_train, num_words = train_loop(train_loader, optimizer, prediction_loss, model)\n",
        "\n",
        "        # Cross entropy\n",
        "        ce_train = np.asarray(loss_train).sum() / np.asarray(num_words).sum()\n",
        "        print('CE on the training set: ', ce_train, '\\n')\n",
        "\n",
        "        # Perplexity\n",
        "        perplexity_train = np.exp(ce_train)\n",
        "        print('Perplexity on the training set: ', perplexity_train, '\\n')\n",
        "\n",
        "        if x % 3 == 0:\n",
        "\n",
        "            sampled_epochs.append(x)\n",
        "            losses_train.append(ce_train)\n",
        "\n",
        "            \"\"\" Cross entropy loss and perplexity on the validation set \"\"\"\n",
        "            loss_valid, num_words = eval_loop(valid_loader, prediction_loss, model)\n",
        "\n",
        "            # Cross entropy\n",
        "            ce_valid = np.asarray(loss_valid).sum() / np.asarray(num_words).sum()\n",
        "            print('CE on the validation set: ', ce_valid, '\\n')\n",
        "\n",
        "            # Perplexity\n",
        "            perplexity_valid = np.exp(ce_valid)\n",
        "            print('Perplexity on the validation set: ', perplexity_valid, '\\n')\n",
        "\n",
        "            losses_valid.append(ce_valid)\n",
        "\n",
        "            if ce_valid < best_loss:\n",
        "                best_loss = ce_valid\n",
        "            else:\n",
        "                patience -= 1\n",
        "            if patience <= 0: \n",
        "                # Early stopping with patience\n",
        "                break \n",
        "\n",
        "\n",
        "    \"\"\" Cross entropy loss and perplexity on the test set \"\"\"\n",
        "    loss_test, num_words = eval_loop(test_loader, prediction_loss, model)\n",
        "\n",
        "    # Cross entropy\n",
        "    ce_test = np.asarray(loss_test).sum() / np.asarray(num_words).sum()\n",
        "    print('CE on the test set: ', ce_test, '\\n')\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity_test = np.exp(ce_test)\n",
        "    print('Perplexity on the test set: ', perplexity_test, '\\n')\n",
        "\n",
        "\n",
        "    return losses_train, losses_valid, sampled_epochs, model\n"
      ],
      "metadata": {
        "id": "JONz8aR3-leA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Training of the neural network\n",
        "\n",
        "In order to train the improved LSTM Language Models, the previously defined **main** procedure is invoked, but some important elements and hyperparameters introduced or changed:\n",
        "\n",
        "*  the **model architecture** corresponds now to the newly defined one: in this case the hidden size and embedding size coincide, otherwise it would not be possible to put in place **weight tying**. Their dimension is set to 500 in order to increase the model's **capacity**.\n",
        "\n",
        "*  The **optimizer**: I tried training the model both using ASGD, as suggested by the *Regularizing and Optimizing LSTM Language Models* paper, and Adam. For my model, **Adam** optimization resulted in being the best performing one among the two. The learning rate **lr** is initialized to the value that has shown to work well in practice, and **weight decay** regularization is introduced along with the optimization procedure.\n",
        "\n",
        "*   The **clip** hyperparameter defined the threshold above which to perform gradient norm clipping. Its value was set to 0.25, as proposed in the *Regularizing and Optimizing LSTM Language Models* paper.\n",
        "\n",
        "In order to show the improvements in the performance brought by the introduction of the **attention module**, the improved LSTM model is trained and evaluated in two cases: with and without the presence of the attention layer. Its introduction in the architecture can be regulated through a simple binary input parameter in the definition of the model. \n",
        "\n",
        "<br/>\n",
        "\n",
        "1.  **Improved model WITHOUT attention**"
      ],
      "metadata": {
        "id": "uaiTPa8DcjjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters \n",
        "n_epochs = 200        # Number of epochs\n",
        "patience = 3          # Patience parameter for the early stopping regularization\n",
        "hid_size = 500        # Size of the hidden LSTM representation \n",
        "emb_size = 500        # Size of the embedding vector\n",
        "lr = 0.001            # Learning rate for the optimization phase\n",
        "clip = 0.25           # Threshold used to clip the gradient and avoid the exploding gradient problem\n",
        "wd = 1.2e-5           # Weight decay regularization\n",
        "\n",
        "# Computation of the length of the vocabulary \n",
        "vocab_len = len(vocab.word2id)\n",
        "\n",
        "# Definition of the model, initialization of the weights\n",
        "model_noatt = Improved_LSTM(hid_size, emb_size, vocab_len, attention = False, pad_index=PAD_TOKEN, tie_weights=True).to(device)\n",
        "model_noatt.apply(init_weights)\n",
        "\n",
        "# Definition of the optimizer\n",
        "optimizer = torch.optim.Adam(model_noatt.parameters(), lr = lr, weight_decay = wd)\n",
        "#optimizer = torch.optim.ASGD(model_noatt.parameters(), lr = lr, weight_decay=wd)\n",
        "\n",
        "# Definition of the loss\n",
        "prediction_loss = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN, reduction = 'sum')\n",
        "\n",
        "# Training procedure\n",
        "loss_train_noatt, loss_valid_noatt, sampled_epochs_noatt, model_noatt = main(model_noatt, \n",
        "                                                                        prediction_loss,\n",
        "                                                                        optimizer, \n",
        "                                                                        train_loader, \n",
        "                                                                        valid_loader, \n",
        "                                                                        test_loader,\n",
        "                                                                        n_epochs = n_epochs,   \n",
        "                                                                        patience = patience)\n",
        "# Save the trained model \n",
        "torch.save(model_noatt.state_dict(), \"gdrive/MyDrive/NLU_project/models/final_model_noatt\")"
      ],
      "metadata": {
        "id": "ToeEUTvMcjjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "114c88ad-ea48-4b89-d2d6-652c8cdf77cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/199 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch n.  1 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/199 [00:29<1:38:48, 29.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  5.920194669568814 \n",
            "\n",
            "Perplexity on the training set:  372.48421816090377 \n",
            "\n",
            "Epoch n.  2 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 2/199 [00:59<1:37:20, 29.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  5.353861609159008 \n",
            "\n",
            "Perplexity on the training set:  211.42315712005765 \n",
            "\n",
            "Epoch n.  3 \n",
            "\n",
            "CE on the training set:  5.153573609694759 \n",
            "\n",
            "Perplexity on the training set:  173.04879550929178 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 3/199 [01:29<1:37:32, 29.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  5.0517420624988905 \n",
            "\n",
            "Perplexity on the validation set:  156.29450225536107 \n",
            "\n",
            "Epoch n.  4 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 4/199 [01:59<1:37:18, 29.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  5.033255323085079 \n",
            "\n",
            "Perplexity on the training set:  153.431670267705 \n",
            "\n",
            "Epoch n.  5 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 5/199 [02:28<1:36:10, 29.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.951359014487651 \n",
            "\n",
            "Perplexity on the training set:  141.36695317127666 \n",
            "\n",
            "Epoch n.  6 \n",
            "\n",
            "CE on the training set:  4.886910134597233 \n",
            "\n",
            "Perplexity on the training set:  132.54339942118406 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 6/199 [02:59<1:36:06, 29.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.875260362328412 \n",
            "\n",
            "Perplexity on the validation set:  131.00825838596907 \n",
            "\n",
            "Epoch n.  7 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 7/199 [03:28<1:35:11, 29.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.83968817550634 \n",
            "\n",
            "Perplexity on the training set:  126.429921636502 \n",
            "\n",
            "Epoch n.  8 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 8/199 [03:58<1:34:44, 29.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.803760388681202 \n",
            "\n",
            "Perplexity on the training set:  121.96820410572555 \n",
            "\n",
            "Epoch n.  9 \n",
            "\n",
            "CE on the training set:  4.773603875664071 \n",
            "\n",
            "Perplexity on the training set:  118.34497493066131 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▍         | 9/199 [04:28<1:34:49, 29.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.812674349494779 \n",
            "\n",
            "Perplexity on the validation set:  123.06028404525827 \n",
            "\n",
            "Epoch n.  10 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 10/199 [04:58<1:33:48, 29.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.749945898325019 \n",
            "\n",
            "Perplexity on the training set:  115.57803139294731 \n",
            "\n",
            "Epoch n.  11 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 11/199 [05:27<1:33:00, 29.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.728729398929088 \n",
            "\n",
            "Perplexity on the training set:  113.15170030080073 \n",
            "\n",
            "Epoch n.  12 \n",
            "\n",
            "CE on the training set:  4.710891543265299 \n",
            "\n",
            "Perplexity on the training set:  111.15121185894995 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 12/199 [05:57<1:32:55, 29.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.7682983450463485 \n",
            "\n",
            "Perplexity on the validation set:  117.71875472688279 \n",
            "\n",
            "Epoch n.  13 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 13/199 [06:27<1:32:32, 29.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.6969705947409555 \n",
            "\n",
            "Perplexity on the training set:  109.61460190949687 \n",
            "\n",
            "Epoch n.  14 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 14/199 [06:57<1:31:42, 29.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.683995227987173 \n",
            "\n",
            "Perplexity on the training set:  108.20149983278743 \n",
            "\n",
            "Epoch n.  15 \n",
            "\n",
            "CE on the training set:  4.672786844328868 \n",
            "\n",
            "Perplexity on the training set:  106.99550715009322 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 15/199 [07:27<1:31:44, 29.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.75330877660401 \n",
            "\n",
            "Perplexity on the validation set:  115.96736051055511 \n",
            "\n",
            "Epoch n.  16 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 16/199 [07:56<1:30:52, 29.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.6629535632785135 \n",
            "\n",
            "Perplexity on the training set:  105.94854622385154 \n",
            "\n",
            "Epoch n.  17 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▊         | 17/199 [08:26<1:30:09, 29.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.6547024184341375 \n",
            "\n",
            "Perplexity on the training set:  105.07794608612653 \n",
            "\n",
            "Epoch n.  18 \n",
            "\n",
            "CE on the training set:  4.646008368794296 \n",
            "\n",
            "Perplexity on the training set:  104.1683529601021 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 18/199 [08:57<1:30:27, 29.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.733966571706519 \n",
            "\n",
            "Perplexity on the validation set:  113.7458497790467 \n",
            "\n",
            "Epoch n.  19 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 19/199 [09:26<1:29:29, 29.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.6367031820536955 \n",
            "\n",
            "Perplexity on the training set:  103.20354281425355 \n",
            "\n",
            "Epoch n.  20 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 20/199 [09:56<1:28:42, 29.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.632749057708531 \n",
            "\n",
            "Perplexity on the training set:  102.79626890958247 \n",
            "\n",
            "Epoch n.  21 \n",
            "\n",
            "CE on the training set:  4.629239270357868 \n",
            "\n",
            "Perplexity on the training set:  102.4361082786047 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 21/199 [10:26<1:28:32, 29.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.727654376370712 \n",
            "\n",
            "Perplexity on the validation set:  113.03012503024632 \n",
            "\n",
            "Epoch n.  22 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 22/199 [10:56<1:28:07, 29.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.622868866757416 \n",
            "\n",
            "Perplexity on the training set:  101.78562305215809 \n",
            "\n",
            "Epoch n.  23 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 23/199 [11:25<1:27:14, 29.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.615642140731768 \n",
            "\n",
            "Perplexity on the training set:  101.0526977559507 \n",
            "\n",
            "Epoch n.  24 \n",
            "\n",
            "CE on the training set:  4.612286308289339 \n",
            "\n",
            "Perplexity on the training set:  100.71415020656694 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 24/199 [11:55<1:27:02, 29.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.719058517583952 \n",
            "\n",
            "Perplexity on the validation set:  112.06269792704671 \n",
            "\n",
            "Epoch n.  25 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 25/199 [12:25<1:26:19, 29.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.607303047975978 \n",
            "\n",
            "Perplexity on the training set:  100.21351381559792 \n",
            "\n",
            "Epoch n.  26 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 26/199 [12:54<1:25:33, 29.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.604435233315628 \n",
            "\n",
            "Perplexity on the training set:  99.92653173390991 \n",
            "\n",
            "Epoch n.  27 \n",
            "\n",
            "CE on the training set:  4.60033446168875 \n",
            "\n",
            "Perplexity on the training set:  99.51759489915557 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▎        | 27/199 [13:25<1:25:53, 29.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.702830480875524 \n",
            "\n",
            "Perplexity on the validation set:  110.25881666449409 \n",
            "\n",
            "Epoch n.  28 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 28/199 [13:54<1:24:58, 29.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.597208533835347 \n",
            "\n",
            "Perplexity on the training set:  99.20699578548326 \n",
            "\n",
            "Epoch n.  29 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 29/199 [14:24<1:24:09, 29.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.59526373228634 \n",
            "\n",
            "Perplexity on the training set:  99.01424535782506 \n",
            "\n",
            "Epoch n.  30 \n",
            "\n",
            "CE on the training set:  4.592030641252163 \n",
            "\n",
            "Perplexity on the training set:  98.69464022354929 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 30/199 [14:54<1:23:59, 29.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.704808384119948 \n",
            "\n",
            "Perplexity on the validation set:  110.47711374979112 \n",
            "\n",
            "Epoch n.  31 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 31/199 [15:24<1:23:47, 29.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.589619847689723 \n",
            "\n",
            "Perplexity on the training set:  98.45699439286871 \n",
            "\n",
            "Epoch n.  32 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 32/199 [15:54<1:23:01, 29.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.586842669733693 \n",
            "\n",
            "Perplexity on the training set:  98.18394113266997 \n",
            "\n",
            "Epoch n.  33 \n",
            "\n",
            "CE on the training set:  4.584919976348843 \n",
            "\n",
            "Perplexity on the training set:  97.99534488303586 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 33/199 [16:24<1:22:49, 29.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.705938041763967 \n",
            "\n",
            "Perplexity on the validation set:  110.60198558375926 \n",
            "\n",
            "Epoch n.  34 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 34/199 [16:53<1:21:57, 29.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.583081324245039 \n",
            "\n",
            "Perplexity on the training set:  97.81533107809776 \n",
            "\n",
            "Epoch n.  35 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 35/199 [17:23<1:21:13, 29.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.580225754305546 \n",
            "\n",
            "Perplexity on the training set:  97.53641098649679 \n",
            "\n",
            "Epoch n.  36 \n",
            "\n",
            "CE on the training set:  4.576608272093108 \n",
            "\n",
            "Perplexity on the training set:  97.18421217523358 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 35/199 [17:54<1:23:53, 30.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.7036897941380165 \n",
            "\n",
            "Perplexity on the validation set:  110.35360424824181 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the test set:  4.672596721384655 \n",
            "\n",
            "Perplexity on the test set:  106.9751667829028 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "In order to **assess the reliability** of the obtained result for the test loss and perplexity, at this point multiple runs (**5**) of training and test of the aforementioned model are executed, maintaining exactly the same setting described above for each run.  \n",
        "\n",
        "At the end of each run the test loss and perplexity values are stored, and in the end the results' **mean** and **standard deviation** is computed and printed. "
      ],
      "metadata": {
        "id": "mmBD1dbPHxxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the number of runs\n",
        "runs = 5 \n",
        "\n",
        "test_loss_runs, test_perplexity_runs = [], []\n",
        "\n",
        "for x in tqdm(range(0, runs)):\n",
        "\n",
        "    # Hyperparameters \n",
        "    n_epochs = 200        # Number of epochs\n",
        "    patience = 3          # Patience parameter for the early stopping regularization\n",
        "    hid_size = 500        # Size of the hidden LSTM representation \n",
        "    emb_size = 500        # Size of the embedding vector\n",
        "    lr = 0.001            # Learning rate for the optimization phase\n",
        "    clip = 0.25           # Threshold used to clip the gradient and avoid the exploding gradient problem\n",
        "    wd = 1.2e-5           # Weight decay regularization\n",
        "\n",
        "    # Computation of the length of the vocabulary \n",
        "    vocab_len = len(vocab.word2id)\n",
        "\n",
        "    # Definition of the model, initialization of the weights\n",
        "    model_noatt = Improved_LSTM(hid_size, emb_size, vocab_len, attention = False, pad_index=PAD_TOKEN, tie_weights=True).to(device)\n",
        "    model_noatt.apply(init_weights)\n",
        "\n",
        "    # Definition of the optimizer\n",
        "    optimizer = torch.optim.Adam(model_noatt.parameters(), lr = lr, weight_decay = wd)\n",
        "\n",
        "    # Definition of the loss\n",
        "    prediction_loss = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN, reduction = 'sum')\n",
        "    \n",
        "\n",
        "    # Initial best loss value, used for the early stopping technique\n",
        "    best_loss = 100.00\n",
        "\n",
        "\n",
        "    for x in range(1, n_epochs):\n",
        "\n",
        "        # For each epoch:\n",
        "        \"\"\" Training step \"\"\"\n",
        "        loss_train, num_words = train_loop(train_loader, optimizer, prediction_loss, model_noatt)\n",
        "\n",
        "        if x % 3 == 0:\n",
        "\n",
        "            \"\"\" Validation step \"\"\"\n",
        "            loss_valid, num_words = eval_loop(valid_loader, prediction_loss, model_noatt)\n",
        "\n",
        "            # Cross entropy\n",
        "            ce_valid = np.asarray(loss_valid).sum() / np.asarray(num_words).sum()\n",
        "\n",
        "            if ce_valid < best_loss:\n",
        "                best_loss = ce_valid\n",
        "            else:\n",
        "                patience -= 1\n",
        "            if patience <= 0: \n",
        "                # Early stopping with patience\n",
        "                break \n",
        "\n",
        "\n",
        "    \"\"\" Cross entropy loss and perplexity on the test set \"\"\"\n",
        "    loss_test, num_words = eval_loop(test_loader, prediction_loss, model_noatt)\n",
        "\n",
        "    # Cross entropy\n",
        "    ce_test = np.asarray(loss_test).sum() / np.asarray(num_words).sum()\n",
        "    test_loss_runs.append(ce_test)\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity_test = np.exp(ce_test)\n",
        "    test_perplexity_runs.append(perplexity_test)\n",
        "    print(perplexity_test)\n",
        "\n",
        "test_loss_runs = np.asarray(test_loss_runs)\n",
        "test_perplexity_runs = np.asarray(test_perplexity_runs)\n",
        "\n",
        "# Computation of mean and standard deviation values among the results of the 5 runs\n",
        "print('Test loss', round(test_loss_runs.mean(),3), '+-', round(test_loss_runs.std(),3))\n",
        "print('Test perplexity', round(test_perplexity_runs.mean(), 3), '+-', round(test_perplexity_runs.std(), 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68289818-743b-4dbe-917b-a1f7b2d31daf",
        "id": "Om01OwqPHxxe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [29:46<1:59:05, 1786.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "106.0367830443064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [1:03:09<1:35:42, 1914.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105.69032445965743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [1:34:47<1:03:33, 1906.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105.23669994845878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [2:06:31<31:45, 1905.55s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105.74162403878158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [2:34:31<00:00, 1854.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105.96736698019647\n",
            "Test loss 4.661 +- 0.003\n",
            "Test perplexity 105.735 +- 0.281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "\n",
        "2.  **Improved model WITH attention**"
      ],
      "metadata": {
        "id": "cDjW0GHieHCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters \n",
        "n_epochs = 200        # Number of epochs\n",
        "patience = 3          # Patience parameter for the early stopping regularization\n",
        "hid_size = 500        # Size of the hidden LSTM representation \n",
        "emb_size = 500        # Size of the embedding vector\n",
        "lr = 0.001            # Learning rate for the optimization phase\n",
        "clip = 0.25           # Threshold used to clip the gradient and avoid the exploding gradient problem\n",
        "wd = 1.2e-5           # Weight decay regularization\n",
        "\n",
        "# Computation of the length of the vocabulary \n",
        "vocab_len = len(vocab.word2id)\n",
        "\n",
        "# Definition of the model, initialization of the weights\n",
        "model_att = Improved_LSTM(hid_size, emb_size, vocab_len, attention = True, pad_index=PAD_TOKEN, tie_weights=True).to(device)\n",
        "model_att.apply(init_weights)\n",
        "\n",
        "# Definition of the optimizer\n",
        "optimizer = torch.optim.Adam(model_att.parameters(), lr = lr, weight_decay = wd)\n",
        "#optimizer = torch.optim.ASGD(model_att.parameters(), lr = lr, weight_decay=wd)\n",
        "\n",
        "# Definition of the loss\n",
        "prediction_loss = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN, reduction = 'sum')\n",
        "\n",
        "# Training procedure\n",
        "loss_train_att, loss_valid_att, sampled_epochs_att, model_att = main(model_att,\n",
        "                                                                    prediction_loss, \n",
        "                                                                    optimizer, \n",
        "                                                                    train_loader, \n",
        "                                                                    valid_loader, \n",
        "                                                                    test_loader,\n",
        "                                                                    n_epochs = n_epochs,   \n",
        "                                                                    patience = patience)\n",
        "# Save the trained model \n",
        "torch.save(model_att.state_dict(), \"gdrive/MyDrive/NLU_project/models/final_model_att\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783e54a2-329c-47bd-8ec5-1d946a6fbee0",
        "id": "QSV9fE7QweSn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/199 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch n.  1 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/199 [02:11<7:12:44, 131.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  5.8135714651943085 \n",
            "\n",
            "Perplexity on the training set:  334.812765021987 \n",
            "\n",
            "Epoch n.  2 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 2/199 [04:22<7:10:16, 131.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  5.185196735459563 \n",
            "\n",
            "Perplexity on the training set:  178.60858497559096 \n",
            "\n",
            "Epoch n.  3 \n",
            "\n",
            "CE on the training set:  4.969149752022566 \n",
            "\n",
            "Perplexity on the training set:  143.9044808465604 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 3/199 [06:34<7:09:43, 131.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.897575512601666 \n",
            "\n",
            "Perplexity on the validation set:  133.96459017459165 \n",
            "\n",
            "Epoch n.  4 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 4/199 [08:45<7:07:36, 131.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.838642782185973 \n",
            "\n",
            "Perplexity on the training set:  126.29782170115413 \n",
            "\n",
            "Epoch n.  5 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 5/199 [10:56<7:04:49, 131.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.752013655484818 \n",
            "\n",
            "Perplexity on the training set:  115.8172659491191 \n",
            "\n",
            "Epoch n.  6 \n",
            "\n",
            "CE on the training set:  4.691887587848604 \n",
            "\n",
            "Perplexity on the training set:  109.05884379025495 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 6/199 [13:09<7:04:23, 131.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.74847043591952 \n",
            "\n",
            "Perplexity on the validation set:  115.40762609694363 \n",
            "\n",
            "Epoch n.  7 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 7/199 [15:20<7:01:14, 131.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.645875164269331 \n",
            "\n",
            "Perplexity on the training set:  104.15447818824138 \n",
            "\n",
            "Epoch n.  8 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 8/199 [17:32<6:58:42, 131.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.611623496136624 \n",
            "\n",
            "Perplexity on the training set:  100.64741776184042 \n",
            "\n",
            "Epoch n.  9 \n",
            "\n",
            "CE on the training set:  4.5837941206267665 \n",
            "\n",
            "Perplexity on the training set:  97.88507834701508 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▍         | 9/199 [19:44<6:57:18, 131.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.706641634304189 \n",
            "\n",
            "Perplexity on the validation set:  110.67983169851111 \n",
            "\n",
            "Epoch n.  10 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 10/199 [21:55<6:54:00, 131.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.561781558455509 \n",
            "\n",
            "Perplexity on the training set:  95.75391916550363 \n",
            "\n",
            "Epoch n.  11 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 11/199 [24:06<6:51:12, 131.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.541545452516741 \n",
            "\n",
            "Perplexity on the training set:  93.83570674662835 \n",
            "\n",
            "Epoch n.  12 \n",
            "\n",
            "CE on the training set:  4.525375504026779 \n",
            "\n",
            "Perplexity on the training set:  92.33058982829021 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 12/199 [26:18<6:49:59, 131.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.678245575432412 \n",
            "\n",
            "Perplexity on the validation set:  107.58116386991914 \n",
            "\n",
            "Epoch n.  13 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 13/199 [28:28<6:46:41, 131.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.514084126451716 \n",
            "\n",
            "Perplexity on the training set:  91.29391403992824 \n",
            "\n",
            "Epoch n.  14 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 14/199 [30:39<6:43:45, 130.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.503381164768302 \n",
            "\n",
            "Perplexity on the training set:  90.3220091841442 \n",
            "\n",
            "Epoch n.  15 \n",
            "\n",
            "CE on the training set:  4.49399627537106 \n",
            "\n",
            "Perplexity on the training set:  89.47831231118829 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 15/199 [32:50<6:41:45, 131.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.6624324806713044 \n",
            "\n",
            "Perplexity on the validation set:  105.893352660607 \n",
            "\n",
            "Epoch n.  16 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 16/199 [34:59<6:38:10, 130.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.484543235558992 \n",
            "\n",
            "Perplexity on the training set:  88.63645558418658 \n",
            "\n",
            "Epoch n.  17 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▊         | 17/199 [37:09<6:35:39, 130.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.474880082980931 \n",
            "\n",
            "Perplexity on the training set:  87.78407297420529 \n",
            "\n",
            "Epoch n.  18 \n",
            "\n",
            "CE on the training set:  4.467718803301583 \n",
            "\n",
            "Perplexity on the training set:  87.15767226858853 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 18/199 [39:21<6:34:32, 130.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.6619017947717625 \n",
            "\n",
            "Perplexity on the validation set:  105.83717146009877 \n",
            "\n",
            "Epoch n.  19 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 19/199 [41:31<6:31:51, 130.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.463512400250985 \n",
            "\n",
            "Perplexity on the training set:  86.79182196642927 \n",
            "\n",
            "Epoch n.  20 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 20/199 [43:41<6:29:04, 130.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.455925521167438 \n",
            "\n",
            "Perplexity on the training set:  86.13583450310182 \n",
            "\n",
            "Epoch n.  21 \n",
            "\n",
            "CE on the training set:  4.452342669405865 \n",
            "\n",
            "Perplexity on the training set:  85.82777477243455 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 21/199 [45:53<6:28:30, 130.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.651059481521479 \n",
            "\n",
            "Perplexity on the validation set:  104.69585015587309 \n",
            "\n",
            "Epoch n.  22 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 22/199 [48:04<6:25:58, 130.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.446031599917721 \n",
            "\n",
            "Perplexity on the training set:  85.28781537466898 \n",
            "\n",
            "Epoch n.  23 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 23/199 [50:14<6:23:20, 130.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.442244938857377 \n",
            "\n",
            "Perplexity on the training set:  84.96547001674575 \n",
            "\n",
            "Epoch n.  24 \n",
            "\n",
            "CE on the training set:  4.436686617232596 \n",
            "\n",
            "Perplexity on the training set:  84.49451468051753 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 24/199 [52:27<6:22:55, 131.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.643160285413189 \n",
            "\n",
            "Perplexity on the validation set:  103.8720948893032 \n",
            "\n",
            "Epoch n.  25 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 25/199 [54:38<6:20:24, 131.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.435627755345097 \n",
            "\n",
            "Perplexity on the training set:  84.40509400964459 \n",
            "\n",
            "Epoch n.  26 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 26/199 [56:49<6:17:49, 131.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.431742128636406 \n",
            "\n",
            "Perplexity on the training set:  84.07776367559404 \n",
            "\n",
            "Epoch n.  27 \n",
            "\n",
            "CE on the training set:  4.427624984853805 \n",
            "\n",
            "Perplexity on the training set:  83.73231505241881 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▎        | 27/199 [59:01<6:17:07, 131.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.651076043128019 \n",
            "\n",
            "Perplexity on the validation set:  104.6975841017082 \n",
            "\n",
            "Epoch n.  28 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 28/199 [1:01:12<6:14:30, 131.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.425812810945135 \n",
            "\n",
            "Perplexity on the training set:  83.5807149401398 \n",
            "\n",
            "Epoch n.  29 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 29/199 [1:03:23<6:11:40, 131.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.421151033804241 \n",
            "\n",
            "Perplexity on the training set:  83.19198705917839 \n",
            "\n",
            "Epoch n.  30 \n",
            "\n",
            "CE on the training set:  4.418520194041453 \n",
            "\n",
            "Perplexity on the training set:  82.97340991846441 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 30/199 [1:05:36<6:10:49, 131.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.649643254324123 \n",
            "\n",
            "Perplexity on the validation set:  104.5476819900902 \n",
            "\n",
            "Epoch n.  31 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 31/199 [1:07:47<6:08:02, 131.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.416674550732242 \n",
            "\n",
            "Perplexity on the training set:  82.82041183298337 \n",
            "\n",
            "Epoch n.  32 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 32/199 [1:09:58<6:05:38, 131.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.413904425049715 \n",
            "\n",
            "Perplexity on the training set:  82.59130635511674 \n",
            "\n",
            "Epoch n.  33 \n",
            "\n",
            "CE on the training set:  4.411203398572559 \n",
            "\n",
            "Perplexity on the training set:  82.36852605305786 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 33/199 [1:12:11<6:04:55, 131.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.635888859168561 \n",
            "\n",
            "Perplexity on the validation set:  103.11953601648412 \n",
            "\n",
            "Epoch n.  34 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 34/199 [1:14:22<6:01:58, 131.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.410248169993992 \n",
            "\n",
            "Perplexity on the training set:  82.28988285009494 \n",
            "\n",
            "Epoch n.  35 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 35/199 [1:16:34<5:59:54, 131.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the training set:  4.406826312339189 \n",
            "\n",
            "Perplexity on the training set:  82.00877980614165 \n",
            "\n",
            "Epoch n.  36 \n",
            "\n",
            "CE on the training set:  4.406659845709098 \n",
            "\n",
            "Perplexity on the training set:  81.99512921714474 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 35/199 [1:18:47<6:09:11, 135.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the validation set:  4.639139123876793 \n",
            "\n",
            "Perplexity on the validation set:  103.45524708450233 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CE on the test set:  4.604309276769002 \n",
            "\n",
            "Perplexity on the test set:  99.91394612569295 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "In order to **assess the reliability** of the obtained result for the test loss and perplexity, at this point multiple runs (**5**) of training and test of the aforementioned model are executed, maintaining exactly the same setting described above for each run.  \n",
        "\n",
        "At the end of each run the test loss and perplexity values are stored, and in the end the results' **mean** and **standard deviation** is computed and printed. "
      ],
      "metadata": {
        "id": "a9IaEpbbI3B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of the number of runs\n",
        "runs = 5 \n",
        "\n",
        "test_loss_runs, test_perplexity_runs = [], []\n",
        "\n",
        "for x in tqdm(range(0, runs)):\n",
        "\n",
        "    # Hyperparameters \n",
        "    n_epochs = 200        # Number of epochs\n",
        "    patience = 3          # Patience parameter for the early stopping regularization\n",
        "    hid_size = 500        # Size of the hidden LSTM representation \n",
        "    emb_size = 500        # Size of the embedding vector\n",
        "    lr = 0.001            # Learning rate for the optimization phase\n",
        "    clip = 0.25           # Threshold used to clip the gradient and avoid the exploding gradient problem\n",
        "    wd = 1.2e-5           # Weight decay regularization\n",
        "\n",
        "    # Computation of the length of the vocabulary \n",
        "    vocab_len = len(vocab.word2id)\n",
        "\n",
        "    # Definition of the model, initialization of the weights\n",
        "    model_att = Improved_LSTM(hid_size, emb_size, vocab_len, attention = True, pad_index=PAD_TOKEN, tie_weights=True).to(device)\n",
        "    model_att.apply(init_weights)\n",
        "\n",
        "    # Definition of the optimizer\n",
        "    optimizer = torch.optim.Adam(model_att.parameters(), lr = lr, weight_decay = wd)\n",
        "\n",
        "    # Definition of the loss\n",
        "    prediction_loss = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN, reduction = 'sum')\n",
        "    \n",
        "\n",
        "    # Initial best loss value, used for the early stopping technique\n",
        "    best_loss = 100.00\n",
        "\n",
        "\n",
        "    for x in range(1, n_epochs):\n",
        "\n",
        "        # For each epoch:\n",
        "        \"\"\" Training step \"\"\"\n",
        "        loss_train, num_words = train_loop(train_loader, optimizer, prediction_loss, model_att)\n",
        "\n",
        "        if x % 3 == 0:\n",
        "\n",
        "            \"\"\" Validation step \"\"\"\n",
        "            loss_valid, num_words = eval_loop(valid_loader, prediction_loss, model_att)\n",
        "\n",
        "            # Cross entropy\n",
        "            ce_valid = np.asarray(loss_valid).sum() / np.asarray(num_words).sum()\n",
        "\n",
        "            if ce_valid < best_loss:\n",
        "                best_loss = ce_valid\n",
        "            else:\n",
        "                patience -= 1\n",
        "            if patience <= 0: \n",
        "                # Early stopping with patience\n",
        "                break \n",
        "\n",
        "\n",
        "    \"\"\" Cross entropy loss and perplexity on the test set \"\"\"\n",
        "    loss_test, num_words = eval_loop(test_loader, prediction_loss, model_att)\n",
        "\n",
        "    # Cross entropy\n",
        "    ce_test = np.asarray(loss_test).sum() / np.asarray(num_words).sum()\n",
        "    test_loss_runs.append(ce_test)\n",
        "\n",
        "    # Perplexity\n",
        "    perplexity_test = np.exp(ce_test)\n",
        "    test_perplexity_runs.append(perplexity_test)\n",
        "    print(perplexity_test)\n",
        "\n",
        "\n",
        "test_loss_runs = np.asarray(test_loss_runs)\n",
        "test_perplexity_runs = np.asarray(test_perplexity_runs)\n",
        "\n",
        "# Computation of mean and standard deviation values among the results of the 5 runs\n",
        "print('Test loss', round(test_loss_runs.mean(),3), '+-', round(test_loss_runs.std(),3))\n",
        "print('Test perplexity', round(test_perplexity_runs.mean(), 3), '+-', round(test_perplexity_runs.std(), 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "847ccf39-246e-47e4-ecd7-70b5c1f039f6",
        "id": "XB6fqMQloOJz"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 1/5 [1:13:28<4:53:53, 4408.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.88950504731793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [2:46:57<4:15:44, 5114.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.15998993067198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [4:20:27<2:58:01, 5340.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.31901766844857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [5:53:01<1:30:24, 5424.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99.42241199578274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [6:58:51<00:00, 5026.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.86198328298684\n",
            "Test loss 4.608 +- 0.005\n",
            "Test perplexity 100.331 +- 0.538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "### Plot of the training and validation losses for the improved models\n",
        "\n",
        "The plots clearly show that the improved LSTM architecture performs generally better with respect to the starting baseline models, \n",
        "  -  the final **perplexity values** for the validation and test set are much **lower**,\n",
        "  -  the **training procedure is smoother**, less affected by the problems of vanishing and exploding gradient,\n",
        "  -  the gap between training error and validation/test errors is smaller: the model is **less prone to overfit** and it is characterized by **higher generalization capabilities**.\n"
      ],
      "metadata": {
        "id": "AWzOcswoekMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved LSTM without Attention\n",
        "\n",
        "plt.figure(num = 3, figsize=(8, 5)).patch.set_facecolor('white')\n",
        "plt.title('No Attention LSTM: Train and Valid Losses')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.plot(sampled_epochs_noatt, loss_train_noatt, label='No Attention LSTM: Train loss')\n",
        "plt.plot(sampled_epochs_noatt, loss_valid_noatt, label='No Attention LSTM: Valid loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Improved LSTM with Attention\n",
        "\n",
        "plt.figure(num = 3, figsize=(8, 5)).patch.set_facecolor('white')\n",
        "plt.title('Attention LSTM: Train and Valid Losses')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.plot(sampled_epochs_att, loss_train_att, label='Attention LSTM: Train loss')\n",
        "plt.plot(sampled_epochs_att, loss_valid_att, label='Attention LSTM: Valid loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "f2f811a1-b28b-45b4-b63f-44d7fddb0578",
        "id": "huRFNiO7ekMS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "## <center>Models comparison and error analysis</center>\n",
        "\n",
        "As clearly shown by the obtained results, the **improved LSTM model with attention** is the one that grants the most stable training procedure, with the best resulting model.\n",
        "\n",
        "Although the improvement in Perplexity score with respect to the baseline models is remarkable, the final perplexity value obtained by the improved model on the test set is still quite high, that is around the value of 100. In order to understand the behaviour of the network when it deals with previously unseen sentences, a detailed **error analysis** must be carried out. \n",
        "\n",
        "For the sake of studying the behaviour of the network, a run on all the samples of the test set is performed and the obtained predictions are stored and analyzed. "
      ],
      "metadata": {
        "id": "B6iKKMT90emK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output predictions\n",
        "test_outputs_idx = []\n",
        "test_inputs_idx = []\n",
        "test_targets_idx = []\n",
        "test_outputs = []\n",
        "test_inputs = []\n",
        "test_targets = []\n",
        "test_loss = []\n",
        "lengths = []\n",
        "\n",
        "# Definition of the model\n",
        "hid_size = 500        \n",
        "emb_size = 500 \n",
        "vocab_len = len(vocab.word2id)\n",
        "\n",
        "model = Improved_LSTM(hid_size, emb_size, vocab_len, attention = True, pad_index=PAD_TOKEN, tie_weights=True).to(device)\n",
        "model.load_state_dict(torch.load(\"gdrive/MyDrive/NLU_project/models/final_model_att\"))\n",
        "\n",
        "# Set the network to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define a dataloader for the test set with batch_size = 1\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1, collate_fn=collate_fn)\n",
        "\n",
        "# Definition of the cost function\n",
        "# In this case reduction = 'mean' -> the mean is computed already\n",
        "cost_function = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN)\n",
        "\n",
        "# Disable gradient computation \n",
        "with torch.no_grad():\n",
        "\n",
        "  # Iterate over the test set\n",
        "  for batch_idx, (inputs, targets, length) in enumerate(test_loader):\n",
        "    \n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      length = length.to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs, _, _, _, _ = model(inputs, length)\n",
        "      _, predicted_index = outputs.max(dim = 1)\n",
        "\n",
        "      # Loss computation\n",
        "      loss = cost_function(outputs, targets.view(-1))\n",
        "\n",
        "      # Update of the outputs array\n",
        "      test_outputs_idx.append(predicted_index)\n",
        "      test_inputs_idx.append(inputs)\n",
        "      test_targets_idx.append(targets)\n",
        "      test_loss.append(loss.item())\n",
        "      lengths.append(length.item())\n",
        "\n",
        "# For each sentence in the predictions\n",
        "for sent_idx in test_outputs_idx:\n",
        "\n",
        "      # Convert list of indexes to list of words\n",
        "      sent_idx = sent_idx.tolist()\n",
        "      sent = [vocab.id2word[id] for id in sent_idx]\n",
        "      test_outputs.append(sent)\n",
        "\n",
        "# For each sentence in the inputs\n",
        "for sent_idx in test_inputs_idx:\n",
        "\n",
        "      # Convert list of indexes to list of words\n",
        "      sent_idx = sent_idx.tolist()\n",
        "      sent = [vocab.id2word[id] for id in sent_idx[0]]\n",
        "      test_inputs.append(sent)\n",
        "\n",
        "# For each sentence in the targets\n",
        "for sent_idx in test_targets_idx:\n",
        "\n",
        "      # Convert list of indexes to list of words\n",
        "      sent_idx = sent_idx.tolist()\n",
        "      sent = [vocab.id2word[id] for id in sent_idx[0]]\n",
        "      test_targets.append(sent)\n",
        "\n",
        "# Test triplets: [(input, target, prediction, sentence-level loss, sentence length)]\n",
        "test_tuples = [(test_inputs[i], test_targets[i], test_outputs[i], test_loss[i], lengths[i]) for i in range(len(test_loss))]\n",
        "sorted_tuples = sorted(test_tuples, key=lambda tup: tup[3])"
      ],
      "metadata": {
        "id": "EsQz1U-c0emP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "In order to evaluate the coherence of the results, it may be a good idea  to compute the number of times the model predicts the **'pad'** token. \n",
        "\n",
        "Indeed, **'pad'** is used just for padding and, even though it is contained in the vocabulary and hence could be predicted, it should never be, if the model learns to behave well. As expected, it does not belong to the set of predicted words, and this implies the model behaves well in this sense. "
      ],
      "metadata": {
        "id": "oSTzFF-K0emW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of words in the test set's predictions\n",
        "predicted_words = []\n",
        "for sent in test_outputs:\n",
        "  for w in sent:\n",
        "    predicted_words.append(w) \n",
        "\n",
        "# Set of predicted words\n",
        "predicted_set = set(predicted_words)\n",
        "print(\"'pad' token belongs to the predicted words: \", 'pad' in predicted_set, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f3e627-27ba-4c83-f65c-5ebdf4f8d64f",
        "id": "jTlzh20r0emY"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'pad' token belongs to the predicted words:  False \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "Another starting point to assess the effectiveness of the training could be analyzing whether the most frequently predicted words on the test set are someway aligned to the most frequently appearing words in the training set.\n",
        "\n",
        "Indeed, it is easily verified by analyzing the Frequency Dictionaries that the most frequent predicted words correspond also to the most frequent ones of **both the training and test sets**. This reflects the ability of the model to learn the statistical properties of words appearances from the training data."
      ],
      "metadata": {
        "id": "EzU3D07F0emb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency list of the words in the training set \n",
        "\n",
        "training_freqdist = nltk.FreqDist(train_words_list)\n",
        "print(\"First 50 words of the training Frequency Dictionary: \")\n",
        "sorted_dict_train = dict(sorted(training_freqdist.items(), key=lambda item: item[1], reverse=True)[:50])\n",
        "print(sorted_dict_train, '\\n')\n",
        "\n",
        "# Frequency list of the words in the test set\n",
        "\n",
        "test_freqdist = nltk.FreqDist(test_words_list)\n",
        "print(\"First 50 words of the test Frequency Dictionary: \")\n",
        "print(dict(sorted(test_freqdist.items(), key=lambda item: item[1], reverse=True)[:50]), '\\n')\n",
        "\n",
        "# Frequency list of the predicted words on the test set\n",
        "\n",
        "predicted_freqdist = nltk.FreqDist(predicted_words)\n",
        "print(\"First 50 words of the predicted words' Frequency Dictionary: \")\n",
        "print(dict(sorted(predicted_freqdist.items(), key=lambda item: item[1], reverse=True)[:50]), '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd5a8e7c-2d79-4591-845d-dfe4ed6bfa38",
        "id": "R5hQR0X2McMr"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 50 words of the training Frequency Dictionary: \n",
            "{'the': 50770, '<unk>': 45020, 'N': 32481, 'of': 24400, 'to': 23638, 'a': 21196, 'in': 18000, 'and': 17474, \"'s\": 9784, 'that': 8931, 'for': 8927, '$': 7541, 'is': 7337, 'it': 6112, 'said': 6027, 'on': 5650, 'by': 4915, 'at': 4894, 'as': 4833, 'from': 4724, 'million': 4627, 'with': 4585, 'mr.': 4326, 'was': 4073, 'be': 3923, 'are': 3914, 'its': 3846, 'he': 3632, 'but': 3541, 'has': 3494, 'an': 3477, \"n't\": 3388, 'will': 3270, 'have': 3245, 'new': 2793, 'or': 2704, 'company': 2680, 'they': 2562, 'this': 2438, 'year': 2379, 'which': 2362, 'would': 2308, 'about': 2220, 'says': 2092, 'more': 2065, 'were': 2009, 'market': 2005, 'billion': 1881, 'his': 1852, 'had': 1850} \n",
            "\n",
            "First 50 words of the test Frequency Dictionary: \n",
            "{'<unk>': 4794, 'the': 4529, 'N': 2523, 'of': 2195, 'to': 2042, 'a': 1821, 'in': 1640, 'and': 1539, \"'s\": 903, 'that': 831, 'for': 783, 'is': 667, 'said': 601, '$': 564, 'it': 542, 'on': 507, 'as': 461, 'by': 450, 'at': 420, 'with': 405, 'its': 393, 'was': 391, 'be': 384, 'are': 365, 'from': 355, 'million': 348, \"n't\": 335, 'but': 335, 'mr.': 319, 'have': 309, 'he': 302, 'market': 300, 'has': 296, 'will': 294, 'an': 260, 'about': 258, 'this': 255, 'company': 227, 'or': 220, 'they': 220, 'new': 219, 'year': 214, 'were': 203, 'which': 202, 'more': 198, 'would': 189, 'u.s.': 179, 'stock': 175, 'had': 175, 'than': 173} \n",
            "\n",
            "First 50 words of the predicted words' Frequency Dictionary: \n",
            "{'<unk>': 13935, '</s>': 11846, 'the': 11686, 'of': 3270, 'N': 2944, 'to': 2517, 'a': 1894, 'in': 1826, 'be': 1517, 'said': 1381, \"n't\": 1106, \"'s\": 1101, 'is': 928, 'are': 881, 'and': 801, 'that': 783, '$': 722, 'been': 669, 'company': 645, 'million': 494, 'it': 382, 'have': 347, 'for': 328, 'has': 298, 'by': 296, 'year': 291, 'would': 285, 'was': 255, 'than': 254, 'as': 238, 'market': 226, 'corp.': 217, 'new': 197, 'years': 193, 'with': 176, 'on': 170, 'u.s.': 164, '&': 157, 'inc.': 157, 'first': 155, 'at': 151, 'prices': 144, 'stock': 139, 'its': 135, 'co.': 128, 'share': 126, 'york': 125, 'from': 125, 'quarter': 119, 'months': 115} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "It may also be interesting to test whether the model has learnt how to use **the end-of-sequence token \"\\</s>\"** properly, that is, to mark the  end of a meaningful sentence and the start of a new one. \n",
        "\n",
        "In order to do this, the 50 most frequent predicted words are considered and some plausible **start-of-sentence** words are chosen among them. \n",
        "Starting from these, the network is then used as a generative model, and it is asked to predict the successive word, given the previously predicted one. This generative process stops when the end-of-sequence token **\"\\</s>\"** is reached or more than 100 iterations are executed.\n",
        "\n",
        "Some of the most plausible start-of-sentence words among the ones belonging to the 50 most frequently predicted are: **\"the\"**, **\"N\"**, **\"be\"**, **\"a\"**, **\"\\</s\\>\"**.\n",
        "\n",
        "It is quite clear from the sentences generated by the model that when no proper and significant context is provided, it has a tendency for outputting some of the most frequently seen words in the training phase, such as **\"\\<unk\\>\"**, **\"N\"**, **\"\\</s\\>\"**. It is particularly interesting to notice that, when provided with the end-of-sentence token **\"\\</s\\>\"** as input, the model predicts **\"the\"**, which is a totally valid choice for the start of a new sentence."
      ],
      "metadata": {
        "id": "CGNR0Y_H0eme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For each start-of-sentence token \n",
        "for starting_token in [\"the\", \"N\", \"be\", \"a\", \"</s>\"]:\n",
        "\n",
        "    sent = []\n",
        "\n",
        "    # Map to index format\n",
        "    input = torch.LongTensor([[vocab.word2id[starting_token]]]).to(device)\n",
        "    length = torch.LongTensor([1]).to(device)\n",
        "\n",
        "    # Computation of the output\n",
        "    output, _, _, _, _ = model(input, length)\n",
        "    _, predicted_index = output.max(dim = 1)\n",
        "\n",
        "    # Map to word format\n",
        "    pred_word = vocab.id2word[predicted_index.item()]\n",
        "\n",
        "    # Update the generated sentence\n",
        "    sent.append(pred_word)\n",
        "\n",
        "    t = 0 \n",
        "\n",
        "    # While stopping criteria is not met\n",
        "    while pred_word != \"</s>\" and t < 100:\n",
        "\n",
        "        # Map to index format\n",
        "        input = torch.LongTensor([[vocab.word2id[pred_word]]]).to(device)\n",
        "        length = torch.LongTensor([1]).to(device)\n",
        "\n",
        "        # Computation of the output\n",
        "        output, _, _, _, _ = model(input, length)\n",
        "        _, predicted_index = output.max(dim = 1)\n",
        "\n",
        "        # Map to word format\n",
        "        pred_word = vocab.id2word[predicted_index.item()]\n",
        "\n",
        "        # Update the generated sentence\n",
        "        sent.append(pred_word)\n",
        "        t = t + 1\n",
        "\n",
        "    print(\" \".join(word for word in sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7d1593-01ab-401a-ead5-d23876a8b604",
        "id": "rouAmLnW0emg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> </s>\n",
            "N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N\n",
            "<unk> </s>\n",
            "spokesman </s>\n",
            "the <unk> </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "In order to better perform the error analysis, it could be useful to print and study the characteristics of the test sentences on which the model scored the worst and the best.\n",
        "\n",
        "* **Sentences on which the model obtained the lowest perplexity values**:\n",
        "Very interestingly, of the 30 sentences with lower loss, 18 were already present in the training data. For what concerns the other 12, they are characterized by being very short or, if longer, very repetitive and with no underlying complex structure or meaning. These are the cases in which not too much effort must be put in place to capture and exploit contextual information."
      ],
      "metadata": {
        "id": "uT39zl9Y0emi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(30):\n",
        "    # Extract the tuple\n",
        "    (input, target, output, loss, _) = sorted_tuples[i]\n",
        "\n",
        "    print(\"Input sentence: \", \" \".join(input))\n",
        "    print(\"Target sentence: \", \" \".join(target))\n",
        "    print(\"predicted sentence: \", \" \".join(output))\n",
        "    print(\"Loss: \", loss)\n",
        "    print(\"This sentence also belonged to the training set: \", input in train_sents, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80cf1f66-5ceb-4409-d803-1f00e43cef09",
        "id": "41UyyFls0emj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence:  revenue rose N N to $ N billion from $ N billion\n",
            "Target sentence:  rose N N to $ N billion from $ N billion </s>\n",
            "predicted sentence:  rose N N to $ N billion from $ N billion </s>\n",
            "Loss:  0.22633951902389526\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  N N N to N days N N N to N days N N N to N days N N N to N days N N N to N days N N N to N days N N N to N days N N N to N days\n",
            "Target sentence:  N N to N days N N N to N days N N N to N days N N N to N days N N N to N days N N N to N days N N N to N days N N N to N days </s>\n",
            "predicted sentence:  N </s> </s> N days N N N to N days N N N to N days N N N to N days N N N to N days N N N to N days N N N to N days N N N to N days N\n",
            "Loss:  0.29104653000831604\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  markets\n",
            "Target sentence:  </s>\n",
            "predicted sentence:  </s>\n",
            "Loss:  0.3347003161907196\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  net fell to $ N million or N cents a share from $ N million or N cents a share\n",
            "Target sentence:  fell to $ N million or N cents a share from $ N million or N cents a share </s>\n",
            "predicted sentence:  income N $ N million or $ cents a share from $ N million or N cents a share </s>\n",
            "Loss:  0.4576435089111328\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  revenue gained N N to $ N billion from $ N billion\n",
            "Target sentence:  gained N N to $ N billion from $ N billion </s>\n",
            "predicted sentence:  rose N N to $ N billion from $ N billion </s>\n",
            "Loss:  0.5238772034645081\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  certificates of deposit\n",
            "Target sentence:  of deposit </s>\n",
            "predicted sentence:  of deposit </s>\n",
            "Loss:  0.5286746621131897\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  negotiable bank-backed business credit instruments typically financing an import order\n",
            "Target sentence:  bank-backed business credit instruments typically financing an import order </s>\n",
            "predicted sentence:  </s> business credit instruments typically financing an import order </s>\n",
            "Loss:  0.5655263066291809\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  plunge\n",
            "Target sentence:  </s>\n",
            "predicted sentence:  </s>\n",
            "Loss:  0.5692690014839172\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  annualized average rate of return after expenses for the past N days not a forecast of future returns\n",
            "Target sentence:  average rate of return after expenses for the past N days not a forecast of future returns </s>\n",
            "predicted sentence:  the rate of return after expenses for the past N days not a forecast of future returns </s>\n",
            "Loss:  0.5749097466468811\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  N N standard conventional fixed-rate mortgages N N N rate capped one-year adjustable rate mortgages\n",
            "Target sentence:  N standard conventional fixed-rate mortgages N N N rate capped one-year adjustable rate mortgages </s>\n",
            "predicted sentence:  N </s> conventional fixed-rate mortgages N N N rate capped one-year adjustable rate mortgages </s>\n",
            "Loss:  0.5925795435905457\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  reserves traded among commercial banks for overnight use in amounts of $ N million or more\n",
            "Target sentence:  traded among commercial banks for overnight use in amounts of $ N million or more </s>\n",
            "predicted sentence:  </s> among commercial banks for overnight use in amounts of $ N million or more </s>\n",
            "Loss:  0.625925600528717\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  source telerate systems inc\n",
            "Target sentence:  telerate systems inc </s>\n",
            "predicted sentence:  of systems inc </s>\n",
            "Loss:  0.6420911550521851\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  source telerate systems inc\n",
            "Target sentence:  telerate systems inc </s>\n",
            "predicted sentence:  of systems inc </s>\n",
            "Loss:  0.6420911550521851\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  rated <unk> by moody 's and <unk> by s&p the issue will be sold through underwriters led by morgan stanley & co\n",
            "Target sentence:  <unk> by moody 's and <unk> by s&p the issue will be sold through underwriters led by morgan stanley & co </s>\n",
            "predicted sentence:  <unk> by moody 's and s&p by s&p </s> issue will be sold through underwriters led by merrill stanley & co </s>\n",
            "Loss:  0.6491838693618774\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  revenue rose to $ N million from $ N million\n",
            "Target sentence:  rose to $ N million from $ N million </s>\n",
            "predicted sentence:  rose N $ N billion from $ N million </s>\n",
            "Loss:  0.6612904667854309\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  dow jones industrials N off N transportation N off N utilities N off N\n",
            "Target sentence:  jones industrials N off N transportation N off N utilities N off N </s>\n",
            "predicted sentence:  jones industrial N up N transportation N off N utilities N off N utilities\n",
            "Loss:  0.6672471761703491\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  terms were n't disclosed\n",
            "Target sentence:  were n't disclosed </s>\n",
            "predicted sentence:  of n't disclosed </s>\n",
            "Loss:  0.6909109950065613\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  revenue gained N N to $ N million from $ N million\n",
            "Target sentence:  gained N N to $ N million from $ N million </s>\n",
            "predicted sentence:  rose N N to $ N billion from $ N million </s>\n",
            "Loss:  0.6979773044586182\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  oregon\n",
            "Target sentence:  </s>\n",
            "predicted sentence:  </s>\n",
            "Loss:  0.7785921096801758\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  these rate indications are n't directly comparable lending practices vary widely by location\n",
            "Target sentence:  rate indications are n't directly comparable lending practices vary widely by location </s>\n",
            "predicted sentence:  <unk> indications are n't directly comparable lending practices vary widely by location </s>\n",
            "Loss:  0.7802671194076538\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  $ N million of N N debentures due oct. N N priced at N to yield N N\n",
            "Target sentence:  N million of N N debentures due oct. N N priced at N to yield N N </s>\n",
            "predicted sentence:  N million </s> N N bonds due nov. N N priced at N to yield N N </s>\n",
            "Loss:  0.7864372730255127\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  the average of interbank offered rates for dollar deposits in the london market based on quotations at five major banks\n",
            "Target sentence:  average of interbank offered rates for dollar deposits in the london market based on quotations at five major banks </s>\n",
            "predicted sentence:  <unk> yield interbank offered rates for dollar deposits in london london market based on quotations at five major banks </s>\n",
            "Loss:  0.7867265343666077\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  the following were among friday 's offerings and pricings in the u.s. and non-u.s. capital markets with terms and syndicate manager as compiled by dow jones capital markets report\n",
            "Target sentence:  following were among friday 's offerings and pricings in the u.s. and non-u.s. capital markets with terms and syndicate manager as compiled by dow jones capital markets report </s>\n",
            "predicted sentence:  <unk> the among the 's offerings and pricings in the u.s. and non-u.s. capital markets with terms and syndicate manager as compiled by dow jones capital markets report </s>\n",
            "Loss:  0.8066229820251465\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  in the nine months <unk> 's net rose N N to $ N million or $ N a share from $ N million or $ N a share\n",
            "Target sentence:  the nine months <unk> 's net rose N N to $ N million or $ N a share from $ N million or $ N a share </s>\n",
            "predicted sentence:  the past months net earned net income N N to $ N million or N N a share from $ N million or $ N a share </s>\n",
            "Loss:  0.8360195159912109\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  N N N days N N N days N N N days N N N days N N N days N N N days\n",
            "Target sentence:  N N days N N N days N N N days N N N days N N N days N N N days </s>\n",
            "predicted sentence:  N </s> </s> </s> N N days </s> N N days </s> N N days </s> N N days </s> N N days </s>\n",
            "Loss:  0.8383274674415588\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  source fulton prebon u.s.a inc\n",
            "Target sentence:  fulton prebon u.s.a inc </s>\n",
            "predicted sentence:  of prebon u.s.a inc </s>\n",
            "Loss:  0.9039332270622253\n",
            "This sentence also belonged to the training set:  True \n",
            "\n",
            "Input sentence:  N N N to N N N one month N N N to N N N two months N N N to N N N three months N N N to N N N four months N N N to N N N five months N N N to N N N six months\n",
            "Target sentence:  N N to N N N one month N N N to N N N two months N N N to N N N three months N N N to N N N four months N N N to N N N five months N N N to N N N six months </s>\n",
            "predicted sentence:  N </s> </s> N days </s> </s> month N days </s> to N days N five months N N N to N days N five months N N N to N days N five months N N N to N days N five months N N N to N days N five months </s>\n",
            "Loss:  0.9045479893684387\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  in the latest nine months net income was $ N million or $ N a share on revenue of $ N million\n",
            "Target sentence:  the latest nine months net income was $ N million or $ N a share on revenue of $ N million </s>\n",
            "predicted sentence:  the past period months net income rose $ N million or N N a share </s> revenue of $ N million </s>\n",
            "Loss:  0.9367450475692749\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  results of the tuesday october N N auction of short-term u.s. government bills sold at a discount from face value in units of $ N to $ N million\n",
            "Target sentence:  of the tuesday october N N auction of short-term u.s. government bills sold at a discount from face value in units of $ N to $ N million </s>\n",
            "predicted sentence:  </s> the company october N auction auction of short-term u.s. government bills sold at a discount from face value in units of $ N to $ N million N\n",
            "Loss:  0.9468639492988586\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  average of top rates paid by major new york banks on primary new issues of negotiable c.d.s usually on amounts of $ N million and more\n",
            "Target sentence:  of top rates paid by major new york banks on primary new issues of negotiable c.d.s usually on amounts of $ N million and more </s>\n",
            "predicted sentence:  </s> N rates paid by major banks york banks </s> primary new issues of negotiable c.d.s usually on amounts of $ N million and more </s>\n",
            "Loss:  0.9483597278594971\n",
            "This sentence also belonged to the training set:  True \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "* **Sentences on which the model obtained the highest perplexity values**:\n",
        "Qualitatively, they often contain words that usually appear quite rarely, and supposedly they almost always consist in small cuts of longer periods that are quite difficult to contextualize and make a sense of. For this reason, with no proper context, the model struggles in predicting correctly and just outputs the guesses with the highest overall probability, such as **\"\\<unk\\>\"**, **\"\\</s\\>\"** and **\"the\"**. "
      ],
      "metadata": {
        "id": "33C-V6yO0emk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(30):\n",
        "    # Extract the tuple\n",
        "    (input, target, output, loss, _) = sorted_tuples[-(i + 1)]\n",
        "\n",
        "    print(\"Input sentence: \", \" \".join(input))\n",
        "    print(\"Target sentence: \", \" \".join(target))\n",
        "    print(\"Predicted sentence: \", \" \".join(output))\n",
        "    print(\"Loss: \", loss)\n",
        "    print(\"This sentence also belonged to the training set: \", input in train_sents, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7abe2ee-546d-4505-8ac0-4707bffcc85b",
        "id": "Lp-O6qL-0emm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence:  as commonly understood service implies sacrifice\n",
            "Target sentence:  commonly understood service implies sacrifice </s>\n",
            "Predicted sentence:  the as the </s> that <unk>\n",
            "Loss:  9.352524757385254\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  european drama has had better though still mixed fortunes\n",
            "Target sentence:  drama has had better though still mixed fortunes </s>\n",
            "Predicted sentence:  <unk> <unk> been been than the <unk> the </s>\n",
            "Loss:  7.862180233001709\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  fcc counsel joins firm\n",
            "Target sentence:  counsel joins firm </s>\n",
            "Predicted sentence:  </s> <unk> the of\n",
            "Loss:  7.854410648345947\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  replied a justin salesman exactly\n",
            "Target sentence:  a justin salesman exactly </s>\n",
            "Predicted sentence:  </s> <unk> <unk> in the\n",
            "Loss:  7.8230133056640625\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  <unk> makes and repairs jet engines\n",
            "Target sentence:  makes and repairs jet engines </s>\n",
            "Predicted sentence:  <unk> the <unk> </s> <unk> </s>\n",
            "Loss:  7.7289581298828125\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  some say november\n",
            "Target sentence:  say november </s>\n",
            "Predicted sentence:  <unk> the 's\n",
            "Loss:  7.686651706695557\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  hardest hit are what he calls secondary sites that primarily serve neighborhood residents\n",
            "Target sentence:  hit are what he calls secondary sites that primarily serve neighborhood residents </s>\n",
            "Predicted sentence:  </s> the <unk> the says for </s> </s> the the as <unk> </s>\n",
            "Loss:  7.592613220214844\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  not everything looks grim for quantum\n",
            "Target sentence:  everything looks grim for quantum </s>\n",
            "Predicted sentence:  <unk> </s> like </s> the </s>\n",
            "Loss:  7.554712772369385\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  yes if targeted\n",
            "Target sentence:  if targeted </s>\n",
            "Predicted sentence:  </s> the <unk>\n",
            "Loss:  7.548065185546875\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  but she stressed i am against managed trade\n",
            "Target sentence:  she stressed i am against managed trade </s>\n",
            "Predicted sentence:  the said that think not the to and\n",
            "Loss:  7.536115646362305\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  population drain ends for midwestern states\n",
            "Target sentence:  drain ends for midwestern states </s>\n",
            "Predicted sentence:  </s> </s> in the <unk> </s>\n",
            "Loss:  7.44642972946167\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  the omnibus can be defeated the virus controlled and real service protected\n",
            "Target sentence:  omnibus can be defeated the virus controlled and real service protected </s>\n",
            "Predicted sentence:  <unk> <unk> be <unk> </s> <unk> is by <unk> estate </s> by\n",
            "Loss:  7.3306450843811035\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  these stocks eventually reopened\n",
            "Target sentence:  stocks eventually reopened </s>\n",
            "Predicted sentence:  <unk> are have the\n",
            "Loss:  7.312952518463135\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  what plunge\n",
            "Target sentence:  plunge </s>\n",
            "Predicted sentence:  the in\n",
            "Loss:  7.306215763092041\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  <unk> yes something called a star chamber\n",
            "Target sentence:  yes something called a star chamber </s>\n",
            "Predicted sentence:  <unk> </s> of the <unk> <unk> of\n",
            "Loss:  7.280195713043213\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  all of the gm divisions except cadillac showed big declines\n",
            "Target sentence:  of the gm divisions except cadillac showed big declines </s>\n",
            "Predicted sentence:  <unk> the <unk> 's </s> for <unk> a board in\n",
            "Loss:  7.258247375488281\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  many affluent people place personal success and money above family\n",
            "Target sentence:  affluent people place personal success and money above family </s>\n",
            "Predicted sentence:  <unk> <unk> are the <unk> in <unk> </s> the <unk>\n",
            "Loss:  7.215588569641113\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  <unk> crude carriers ltd.\n",
            "Target sentence:  crude carriers ltd. </s>\n",
            "Predicted sentence:  <unk> </s> </s> said\n",
            "Loss:  7.196537017822266\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  in leipzig activists vowed to continue street protests to demand internal change\n",
            "Target sentence:  leipzig activists vowed to continue street protests to demand internal change </s>\n",
            "Predicted sentence:  the </s> </s> to <unk> to to to the for <unk> </s>\n",
            "Loss:  7.162181854248047\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  the financially struggling station offers programs obviously made available <unk> from its boss 's other ventures\n",
            "Target sentence:  financially struggling station offers programs obviously made available <unk> from its boss 's other ventures </s>\n",
            "Predicted sentence:  <unk> <unk> to </s> a in <unk> the to </s> the <unk> </s> <unk> <unk> </s>\n",
            "Loss:  7.1576056480407715\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  size and weight considerations also have limited screen displays\n",
            "Target sentence:  and weight considerations also have limited screen displays </s>\n",
            "Predicted sentence:  </s> the </s> </s> are been to </s> </s>\n",
            "Loss:  7.152291297912598\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  that is what common cause should ask be investigated\n",
            "Target sentence:  is what common cause should ask be investigated </s>\n",
            "Predicted sentence:  the the the shares </s> be the a </s>\n",
            "Loss:  7.129258155822754\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  canadian investment rules require that big foreign takeovers meet that standard\n",
            "Target sentence:  investment rules require that big foreign takeovers meet that standard </s>\n",
            "Predicted sentence:  <unk> bank </s> a the board banks are with the &\n",
            "Loss:  7.0822553634643555\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  the fcc effort collapsed\n",
            "Target sentence:  fcc effort collapsed </s>\n",
            "Predicted sentence:  <unk> 's to in\n",
            "Loss:  7.07655143737793\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  in fact few consume much of anything\n",
            "Target sentence:  fact few consume much of anything </s>\n",
            "Predicted sentence:  the that years <unk> of the else\n",
            "Loss:  7.067477703094482\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  it might well win senate passage\n",
            "Target sentence:  might well win senate passage </s>\n",
            "Predicted sentence:  <unk> be be the <unk> </s>\n",
            "Loss:  7.066192150115967\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  this role reversal holds true as well for his three liberal and moderate allies justices <unk> marshall harry <unk> and john stevens\n",
            "Target sentence:  role reversal holds true as well for his three liberal and moderate allies justices <unk> marshall harry <unk> and john stevens </s>\n",
            "Predicted sentence:  <unk> in is a interest a as the own years <unk> <unk> <unk> </s> </s> <unk> <unk> <unk> and <unk> <unk> of\n",
            "Loss:  7.052467346191406\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  now producers hope prices have hit bottom\n",
            "Target sentence:  producers hope prices have hit bottom </s>\n",
            "Predicted sentence:  the are to are been the </s>\n",
            "Loss:  7.039435386657715\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  pennsylvania higher education facilities authority\n",
            "Target sentence:  higher education facilities authority </s>\n",
            "Predicted sentence:  </s> </s> and and $\n",
            "Loss:  7.033559322357178\n",
            "This sentence also belonged to the training set:  False \n",
            "\n",
            "Input sentence:  employee benefit plans inc.\n",
            "Target sentence:  benefit plans inc. </s>\n",
            "Predicted sentence:  </s> </s> to a\n",
            "Loss:  6.999269485473633\n",
            "This sentence also belonged to the training set:  False \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "In order to understand how the Perplexity is correlated with the length of the test sequences, I performed two types of analysis on the length-loss pairs. First of all I plotted **how the typical sentence length varies with respect to the loss, for increasing loss values**. Additionally, I computed and visualized the **mean and standard deviation of the loss values for each length** value in the test data.\n",
        "\n",
        "The results are the same in both cases and they show, very interestingly, that while the mean loss value is almost the same for all lengths, the standard deviation tendentially decreases with the increase in length. This happens because longer sequences yield more substantial contextual info, and the performance of the model on them depends on its ability to capture and interpret that info, hence its score is quite stable in this case. For what concerns shorter sentences, instead, as observed during the qualitative analysis, some may be difficult to interpret because of the lack of context, while others can be so repetitive that guessing right is particularly easy; for this reason the model could perform poorly or brilliantly depending on the specific case, and the scores are characterized by high variability. "
      ],
      "metadata": {
        "id": "zoy7l6uI0emn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram that shows how the length of the sentences in the test varies with respect to the associated loss\n",
        "\n",
        "lengths = [tuple[-1] for tuple in sorted_tuples]\n",
        "loss = [tuple[3] for tuple in sorted_tuples]\n",
        "\n",
        "pos = np.linspace(loss[0], loss[-1], 15)\n",
        "width = 0.1  \n",
        "lens_ = np.linspace(loss[0], loss[-1], 15).tolist()\n",
        "lens = [\"{:.1f}\".format(n) for n in lens_]\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xticks(pos + (width / 2))\n",
        "ax.set_xticklabels(lens)\n",
        "ax.set_xlabel(r'Sentence loss')\n",
        "ax.set_ylabel(r'Sentence length')\n",
        "\n",
        "plt.bar(loss, lengths, width, color='m')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "197a9455-620b-4056-85bb-5fc461e35b23",
        "id": "zigVvEnG0emn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_per_len = {i: [] for i in range(1, 78)}\n",
        "for tuple in sorted_tuples:\n",
        "  loss_per_len[tuple[-1]].append(tuple[3])\n",
        "\n",
        "lengths = [i for i in range(1, 78)]\n",
        "mean_per_len = [round(np.mean(loss_per_len[i]), 3) for i in lengths]\n",
        "std_per_len = [round(np.std(loss_per_len[i]), 3) for i in lengths]\n",
        "print(mean_per_len)\n",
        "print(std_per_len)\n",
        "\n",
        "# Histogram that shows the mean of the loss for each test sequence length\n",
        "\n",
        "pos = np.arange(1, 81, 5)\n",
        "width = 0.5  \n",
        "lens = np.arange(1, 81, 5).tolist()\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xticks(pos + (width / 2))\n",
        "ax.set_xticklabels(lens)\n",
        "ax.set_xlabel(r'Sentence length')\n",
        "ax.set_ylabel(r'Mean loss value')\n",
        "\n",
        "plt.bar(lengths, mean_per_len, width, color='g')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Histogram that shows the standard deviation of the loss for each test sentence length\n",
        "\n",
        "pos = np.arange(1, 81, 5)\n",
        "width = 0.5  \n",
        "lens = np.arange(1, 81, 5).tolist()\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xticks(pos + (width / 2))\n",
        "ax.set_xticklabels(lens)\n",
        "ax.set_xlabel(r'Sentence length')\n",
        "ax.set_ylabel(r'Std dev of loss value')\n",
        "\n",
        "plt.bar(lengths, std_per_len, width, color='r')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Histogram that shows the frequency distribution of length values in the test set  \n",
        "\n",
        "lengths = list(range(1, 78))\n",
        "frequencies = [0 for i in lengths]\n",
        "for sent in test_sents:\n",
        "  frequencies[len(sent)-1] += 1\n",
        "\n",
        "pos = np.arange(1, len(lengths)+1, 5)\n",
        "width = 0.5  \n",
        "lens = list(range(1, 81, 5))\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xticks(pos + (width / 2))\n",
        "ax.set_xticklabels(lens)\n",
        "ax.set_xlabel(r'Sentence length')\n",
        "ax.set_ylabel(r'Length frequence in the test set')\n",
        "\n",
        "plt.bar(lengths, frequencies, width, color='b')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KTkkN1f3cvS2",
        "outputId": "0a766e8f-b6c4-4fa4-b97c-c14e347169f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:263: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  keepdims=keepdims, where=where)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in true_divide\n",
            "  subok=False)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.2628668606281281, 4.042672722838645, 4.754617631435394, 4.7452436946332455, 4.689029804120461, 4.660706026737507, 4.805224893523044, 4.729110197175907, 4.82877632856369, 4.634291247679637, 4.619146307309468, 4.489723921649986, 4.450233241833678, 4.503030318324849, 4.48573430418189, 4.616185925550909, 4.805877778566245, 4.610446559854701, 4.6874544431423315, 4.5952237571856775, 4.571715110582663, 4.581495661735534, 4.548211361798975, 4.547730464161488, 4.535211580732595, 4.607835248593362, 4.727936189953644, 4.458623421603236, 4.4563468000323505, 4.587072825147992, 4.722928605862518, 4.637911606479335, 4.547455474734306, 4.7206681655003475, 4.625062530690974, 4.665714706693377, 4.443122516978871, 4.54462447310939, 4.46839702129364, 4.594318548838298, 4.72617503007253, 4.856705599360996, 4.77259227808784, 4.922428982598441, 4.771134734153748, 4.329945683479309, 4.761226654052734, 3.5431721061468124, 4.90362674849374, 4.257231426239014, 4.5473809242248535, 4.751059859991074, 4.495310306549072, 3.2456386238336563, nan, nan, 4.798958778381348, 5.212508201599121, 5.075536251068115, 5.099555492401123, nan, nan, nan, 4.486529350280762, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 4.6146931648254395]\n",
            "[0.9629730454745377, 1.642683745359625, 1.4806061341588563, 1.922986974094744, 1.6159006022958327, 1.6757168776302536, 1.3981333434302923, 1.3962847524267337, 1.2694856062485393, 1.2210368696702354, 1.1480130611541315, 1.3367210606984818, 1.2695362681943547, 1.0574024105398896, 1.095796566089857, 1.0929608048057182, 0.914759154944571, 1.0399854250358957, 0.9336620816899324, 1.0380894776323448, 1.0392990609027148, 1.0751198918181142, 0.9906248096739118, 0.8249444672543674, 0.9629519189848663, 0.8432699136090487, 0.848720690552293, 0.931895531650039, 1.0372803882829718, 0.9436489328349802, 0.7354484075557403, 0.6589557191497357, 0.7901892264215311, 0.8108844357555167, 0.681771009935621, 0.8296125810007271, 0.8190596909533914, 0.9602374280724075, 0.9667224353711887, 0.6005903067221301, 0.5609629227196001, 0.7118644494816372, 0.6249331562574459, 0.5220006772179757, 0.5010126607246991, 0.855306630100509, 0.6311189114510383, 1.7672369856216585, 0.9174545593415044, 0.48423528207028854, 0.4095335006713867, 0.7016193554914695, 0.1579604148864746, 1.6955570692098014, nan, nan, 0.33838796615600586, 0.0, 0.0, 0.13626670837402344, nan, nan, nan, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "Another check that can be performed is considering some of the words that appear most frequently in the **training set**, for example the 50 most frequent words, and compute the percentage of times they are **predicted correctly**, with respect to all the times they appear **in the targets**.\n",
        "\n",
        "Ideally, the more frequently a word appeared during training, the more the model should be able to capture its influence and meaning in different contexts, and the higher should be the percentage of times it is predicted correctly at test time.\n",
        "\n",
        "In order to verify this trend, I decided to plot the percentage of times some words were predicted correctly, when present in the **target sentences of the test set**. The analyzed words are the 50 most frequent words from the training data, considered in **descending order of frequency**. The expected behaviour of the histogram is that a higher **recall** in prediction should be associated to those words that appeared most frequently in the training set, since the model should be able to better capture their meaning and context of appearance. Indeed, a **general decreasing tendency** can be observed from the plot with the decrease of the words' frequencies, except for some outliers characterized by high recall values, although their appearance in the training set is more rare.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Zzu3VOP0ems"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recall = []\n",
        "\n",
        "# For each word in the sorted dictionary of 50 most frequent words in the training set\n",
        "for token in sorted_dict_train:\n",
        "\n",
        "    # Number of appearances in the target set\n",
        "    target_appearance = 0\n",
        "    # Number of correct appearances in the predictions\n",
        "    correct_prediction = 0\n",
        "\n",
        "    # For each sentence in the target set\n",
        "    for i in range(len(test_targets)):\n",
        "\n",
        "        sent = test_targets[i]\n",
        "\n",
        "        # For each word in the sentence\n",
        "        for j in range(len(sent)):\n",
        "            # If the word coincides with the considered token, update target_appearance\n",
        "            if sent[j] == token:\n",
        "                target_appearance += 1\n",
        "                # If the word is correctly predicted, update correct_prediction\n",
        "                if sent[j] == test_outputs[i][j]:\n",
        "                    correct_prediction += 1\n",
        "\n",
        "    # Update list of percentages\n",
        "    recall.append(correct_prediction*100/target_appearance)\n",
        "  \n",
        "\n",
        "\"\"\"\n",
        "The following histogram plots the RECALL value for the classes corresponding\n",
        "to the 50 most frequent words in the training set, considered in decreasing order of frequency\n",
        "\"\"\" \n",
        "frequencies = list(range(1, 51))\n",
        "pos = np.arange(1, 51, 5)\n",
        "width = 0.3  \n",
        "lens = np.arange(1, 51, 5).tolist()\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xticks(pos + (width / 2))\n",
        "ax.set_xticklabels(lens)\n",
        "ax.set_xlabel(r'Frequency rank in training set: 1 = most frequent')\n",
        "ax.set_ylabel(r'Recall in test set')\n",
        "\n",
        "plt.bar(frequencies, recall, width, color='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ZI7oy7XxmixU",
        "outputId": "e9e8b16f-ce0f-45bf-a11f-050626f8a654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br/>\n",
        "\n",
        "Another value that may be interesting to check is the **precision** in the test set corresponding to each of the 50 most frequent words of the training set. The precision corresponds to the percentage of times a certain word is **predicted correctly**, with respect to the number of times it appears **in the predictions**.\n",
        "\n",
        "The computation can be performed in a similar way as it was done for the recall. In this case the plot does not show any meaningful trend."
      ],
      "metadata": {
        "id": "6c4qS5p5ahze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision = []\n",
        "\n",
        "# For each word in the sorted dictionary of 50 most frequent words in the training set\n",
        "for token in sorted_dict_train:\n",
        "\n",
        "    # Number of appearances in the target set\n",
        "    output_appearance = 0\n",
        "    # Number of correct appearances in the predictions\n",
        "    correct_prediction = 0\n",
        "\n",
        "    # For each sentence in the output set\n",
        "    for i in range(len(test_outputs)):\n",
        "\n",
        "        sent = test_outputs[i]\n",
        "\n",
        "        # For each word in the sentence\n",
        "        for j in range(len(sent)):\n",
        "            # If the word coincides with the considered token, update output_appearance\n",
        "            if sent[j] == token:\n",
        "                output_appearance += 1\n",
        "                # If the word is correctly predicted, update correct_prediction\n",
        "                if sent[j] == test_targets[i][j]:\n",
        "                    correct_prediction += 1\n",
        "\n",
        "    # Update list of percentages\n",
        "    precision.append(correct_prediction*100/output_appearance)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The following histogram plots the PRECISION value for the classes corresponding\n",
        "to the 50 most frequent words in the training set, considered in decreasing order of frequency\n",
        "\"\"\" \n",
        "frequencies = list(range(1, 51))\n",
        "pos = np.arange(1, 51, 5)\n",
        "width = 0.3  \n",
        "lens = np.arange(1, 51, 5).tolist()\n",
        "\n",
        "ax = plt.axes()\n",
        "ax.set_xticks(pos + (width / 2))\n",
        "ax.set_xticklabels(lens)\n",
        "ax.set_xlabel(r'Frequency rank in training set: 1 = most frequent')\n",
        "ax.set_ylabel(r'Precision in test set')\n",
        "\n",
        "plt.bar(frequencies, precision, width, color='m')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "GxAE-mJnbvZ9",
        "outputId": "624082d2-8659-4ad3-9706-94af7c06cbe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}